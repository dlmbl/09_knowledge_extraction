{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79694f49",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "# Exercise 8: Knowledge Extraction from a Pre-trained Neural Network\n",
    "\n",
    "The goal of this exercise is to learn how to probe what a pre-trained classifier has learned about the data it was trained on.\n",
    "\n",
    "We will be working with a simple example which is a fun derivation on the MNIST dataset that you will have seen in previous exercises in this course.\n",
    "Unlike regular MNIST, our dataset is classified not by number, but by color!\n",
    "\n",
    "We will:\n",
    "1. Load a pre-trained classifier and try applying conventional attribution methods\n",
    "2. Train a GAN to create counterfactual images - translating images from one class to another\n",
    "3. Evaluate the GAN - see how good it is at fooling the classifier\n",
    "4. Create attributions from the counterfactual, and learn the differences between the classes.\n",
    "\n",
    "If time permits, we will try to apply this all over again as a bonus exercise to a much more complex and more biologically relevant problem.\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook was written by Diane Adjavon, from a previous version written by Jan Funke and modified by Tri Nguyen, using code from Nils Eckstein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa6b82",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>08_knowledge_extraction</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3155a7a",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Setup\n",
    "\n",
    "In this part of the notebook, we will load the same dataset as in the previous exercise.\n",
    "We will also learn to load one of our trained classifiers from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7ad8d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "from classifier.data import ColoredMNIST\n",
    "\n",
    "mnist = ColoredMNIST(\"data\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06eec3e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Some information about the dataset:\n",
    "- The dataset is a colored version of the MNIST dataset.\n",
    "- Instead of using the digits as classes, we use the colors.\n",
    "- There are four classes - the goal of the exercise is to find out what these are.\n",
    "\n",
    "Let's plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    x, y = mnist[i]\n",
    "    x = x.permute((1, 2, 0))  # make channels last\n",
    "    ax.imshow(x)\n",
    "    ax.set_title(f\"Class {y}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dce21e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We have pre-traiend a classifier for you on this dataset. It is the same architecture classifier as you used in the Failure Modes exercise: a `DenseModel`.\n",
    "Let's load that classifier now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cb15d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1.1: Load the classifier</h3>\n",
    "We have written a slightly more general version of the `DenseModel` that you used in the previous exercise. Ours requires two inputs:\n",
    "- `input_shape`: the shape of the input images, as a tuple\n",
    "- `num_classes`: the number of classes in the dataset\n",
    "\n",
    "Create a dense model with the right inputs and load the weights from the checkpoint.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20753b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from classifier.model import DenseModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = DenseModel(input_shape=(3, 28, 28), num_classes=4)\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"extras/checkpoints/model.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc47df",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Don't take my word for it! Let's see how well the classifier does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69dbe4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "test_mnist = ColoredMNIST(\"data\", download=True, train=False)\n",
    "dataloader = DataLoader(test_mnist, batch_size=32, shuffle=False)\n",
    "\n",
    "labels = []\n",
    "predictions = []\n",
    "for x, y in dataloader:\n",
    "    pred = model(x.to(device))\n",
    "    labels.extend(y.cpu().numpy())\n",
    "    predictions.extend(pred.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(labels, predictions, normalize=\"true\")\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a85b2",
   "metadata": {},
   "source": [
    "# Part 2: Using Integrated Gradients to find what the classifier knows\n",
    "\n",
    "In this section we will make a first attempt at highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2126d",
   "metadata": {},
   "source": [
    "## Attributions through integrated gradients\n",
    "\n",
    "Attribution is the process of finding out, based on the output of a neural network, which pixels in the input are (most) responsible. Another way of thinking about it is: which pixels would need to change in order for the network's output to change.\n",
    "\n",
    "Here we will look at an example of an attribution method called [Integrated Gradients](https://captum.ai/docs/extension/integrated_gradients). If you have a bit of time, have a look at this [super fun exploration of attribution methods](https://distill.pub/2020/attribution-baselines/), especially the explanations on Integrated Gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467138c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = [mnist[i] for i in range(batch_size)]\n",
    "x = torch.stack([b[0] for b in batch])\n",
    "y = torch.tensor([b[1] for b in batch])\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e59271",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.1 Get an attribution</h3>\n",
    "\n",
    "In this next part, we will get attributions on single batch. We use a library called [captum](https://captum.ai), and focus on the `IntegratedGradients` method.\n",
    "Create an `IntegratedGradients` object and run attribution on `x,y` obtained above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275108f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Solution for Task 2.1 #\n",
    "#########################\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Create an integrated gradients object.\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = integrated_gradients.attribute(x, target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c56b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributions = (\n",
    "    attributions.cpu().numpy()\n",
    ")  # Move the attributions from the GPU to the CPU, and turn then into numpy arrays for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c7c4d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "Here is an example for an image, and its corresponding attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb988a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_attribution(attribution, original_image):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "\n",
    "    viz.visualize_image_attr_multiple(\n",
    "        attribution,\n",
    "        original_image,\n",
    "        methods=[\"original_image\", \"heat_map\"],\n",
    "        signs=[\"all\", \"absolute_value\"],\n",
    "        show_colorbar=True,\n",
    "        titles=[\"Image\", \"Attribution\"],\n",
    "        use_pyplot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcc258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b40ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "The attributions are shown as a heatmap. The brighter the pixel, the more important this attribution method thinks that it is.\n",
    "As you can see, it is pretty good at recognizing the number within the image.\n",
    "As we know, however, it is not the digit itself that is important for the classification, it is the color!\n",
    "Although the method is picking up really well on the region of interest, it would be difficult to conclude from this that it is the color that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7a6ce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Something is slightly unfair about this visualization though.\n",
    "We are visualizing as if it were grayscale, but both our images and our attributions are in color!\n",
    "Can we learn more from the attributions if we visualize them in color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1324bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_color_attribution(attribution, original_image):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(original_image)\n",
    "    ax1.set_title(\"Image\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(np.abs(attribution))\n",
    "    ax2.set_title(\"Attribution\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "    visualize_color_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16396f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We get some better clues when looking at the attributions in color.\n",
    "The highlighting doesn't just happen in the region with number, but also seems to hapen in a channel that matches the color of the image.\n",
    "Just based on this, however, we don't get much more information than we got from the images themselves.\n",
    "\n",
    "If we didn't know in advance, it is unclear whether the color or the number is the most important feature for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad51e5",
   "metadata": {},
   "source": [
    "\n",
    "### Changing the basline\n",
    "\n",
    "Many existing attribution algorithms are comparative: they show which pixels of the input are responsible for a network output *compared to a baseline*.\n",
    "The baseline is often set to an all 0 tensor, but the choice of the baseline affects the output.\n",
    "(For an interactive illustration of how the baseline affects the output, see [this Distill paper](https://distill.pub/2020/attribution-baselines/))\n",
    "\n",
    "You can change the baseline used by the `integrated_gradients` object.\n",
    "\n",
    "Use the command:\n",
    "```\n",
    "?integrated_gradients.attribute\n",
    "```\n",
    "To get more details about how to include the baseline.\n",
    "\n",
    "Try using the code above to change the baseline and see how this affects the output.\n",
    "\n",
    "1. Random noise as a baseline\n",
    "2. A blurred/noisy version of the original image as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac88671",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.3: Use random noise as a baseline</h4>\n",
    "\n",
    "Hint: `torch.rand_like`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2c935",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Solution for task 2.3 #\n",
    "#########################\n",
    "# Baseline\n",
    "random_baselines = torch.rand_like(x)\n",
    "# Generate the attributions\n",
    "attributions_random = integrated_gradients.attribute(\n",
    "    x, target=y, baselines=random_baselines\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_random.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_color_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d5ceb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.4: Use a blurred image a baseline</h4>\n",
    "\n",
    "Hint: `torchvision.transforms.functional` has a useful function for this ;)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845266ff",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Solution for task 2.4 #\n",
    "#########################\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "\n",
    "# Baseline\n",
    "blurred_baselines = gaussian_blur(x, kernel_size=(5, 5))\n",
    "# Generate the attributions\n",
    "attributions_blurred = integrated_gradients.attribute(\n",
    "    x, target=y, baselines=blurred_baselines\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_blurred.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_color_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3f09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"altert alert-block alert-warning\"><h4> Questions </h4>\n",
    "<ul>\n",
    "<li>What baseline do you like best so far? Why?</li>\n",
    "<li>Why do you think some baselines work better than others?</li>\n",
    "<li>If you were to design an ideal baseline, what would you choose?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a9879",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h2>BONUS Task: Using different attributions.</h2>\n",
    "\n",
    "\n",
    "[`captum`](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) has access to various different attribution algorithms.\n",
    "\n",
    "Replace `IntegratedGradients` with different attribution methods. Are they consistent with each other?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2509232",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 2</h2>\n",
    "Let us know on the exercise chat when you've reached this point!\n",
    "\n",
    "At this point we have:\n",
    "\n",
    "- Loaded a classifier that classifies MNIST-like images by color, but we don't know how!\n",
    "- Tried applying Integrated Gradients to find out what the classifier is looking at - with little success.\n",
    "- Discovered the effect of changing the baseline on the output of integrated gradients.\n",
    "\n",
    "Coming up in the next section, we will learn how to create counterfactual images.\n",
    "These images will change *only what is necessary* in order to change the classification of the image.\n",
    "We'll see that using counterfactuals we will be able to disambiguate between color and number as an important feature.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c1792",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 3: Train a GAN to Translate Images\n",
    "\n",
    "To gain insight into how the trained network classify images, we will use [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412), a feature attribution with counterfactual explanations methodology.\n",
    "This method employs a StarGAN to translate images from one class to another to make counterfactual explanations.\n",
    "\n",
    "**What is a counterfactual?**\n",
    "\n",
    "You've learned about adversarial examples in the lecture on failure modes. These are the imperceptible or noisy changes to an image that drastically changes a classifier's opinion.\n",
    "Counterfactual explanations are the useful cousins of adversarial examples. They are *perceptible* and *informative* changes to an image that changes a classifier's opinion.\n",
    "\n",
    "In the image below you can see the difference between the two. In the first column are MNIST images along with their classifictaions, and in the second column are counterfactual explanations to *change* that class. You can see that in both cases a human being would (hopefully) agree with the new classification. By comparing the two columns, we can therefore begin to define what makes each digit special.\n",
    "\n",
    "In contrast, the third and fourth columns show an MNIST image and a corresponding adversarial example. Here the network returns a prediction that most human beings (who aren't being facetious) would strongly disagree with.\n",
    "\n",
    "<img src=\"assets/ce_vs_ae.png\" width=50% />\n",
    "\n",
    "**Counterfactual synapses**\n",
    "\n",
    "In this example, we will train a StarGAN network that is able to take any of our special MNIST images and change its class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931e876",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "### The model\n",
    "![stargan.png](assets/stargan.png)\n",
    "\n",
    "In the following, we create a [StarGAN model](https://arxiv.org/abs/1711.09020).\n",
    "It is a Generative Adversarial model that is trained to turn one class of images X into a different class of images Y.\n",
    "\n",
    "We will not be using the random latent code (green, in the figure), so the model we use is made up of three networks:\n",
    "- The generator - this will be the bulk of the model, and will be responsible for transforming the images: we're going to use a `UNet`\n",
    "- The discriminator - this will be responsible for telling the difference between real and fake images: we're going to use a `DenseModel`\n",
    "- The style encoder - this will be responsible for encoding the style of the image: we're going to use a `DenseModel`\n",
    "\n",
    "Let's start by creating these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlmbl_unet import UNet\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, generator, style_encoder):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.style_encoder = style_encoder\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor\n",
    "            The source image\n",
    "        y: torch.Tensor\n",
    "            The style image\n",
    "        \"\"\"\n",
    "        style = self.style_encoder(y)\n",
    "        # Concatenate the style vector with the input image\n",
    "        style = style.unsqueeze(-1).unsqueeze(-1)\n",
    "        style = style.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, style], dim=1)\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021b8eb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3.1: Create the models</h3>\n",
    "\n",
    "We are going to create the models for the generator, discriminator, and style mapping.\n",
    "\n",
    "Given the Generator structure above, fill in the missing parts for the unet and the style mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12536b57",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Here is an example of a working setup! Note that you can change the hyperparameters as you experiment.\n",
    "# Choose your own setup to see what works for you.\n",
    "style_encoder = DenseModel(input_shape=(3, 28, 28), num_classes=3)\n",
    "unet = UNet(depth=2, in_channels=6, out_channels=3, final_activation=nn.Sigmoid())\n",
    "generator = Generator(unet, style_encoder=style_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2ece2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Hyper-parameter choices</h3>\n",
    "<ul>\n",
    "<li>Are any of the hyperparameters you choose above constrained in some way?</li>\n",
    "<li>What would happen if you chose a depth of 10 for the UNet?</li>\n",
    "<li>Is there a minimum size for the style space? Why or why not?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f8fb8",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3.2: Create the discriminator</h3>\n",
    "\n",
    "We want the discriminator to be like a classifier, so it is able to look at an image and tell not only whether it is real, but also which class it came from.\n",
    "The discriminator will take as input either a real image or a fake image.\n",
    "Fill in the following code to create a discriminator that can classify the images into the correct number of classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d63a1",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "discriminator = DenseModel(input_shape=(3, 28, 28), num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3019ab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's move all models onto the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5b4b0",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "## Training a GAN\n",
    "\n",
    "Training an adversarial network is a bit more complicated than training a classifier.\n",
    "For starters, we are simultaneously training two different networks that work against each other.\n",
    "As such, we need to be careful about how and when we update the weights of each network.\n",
    "\n",
    "We will have two different optimizers, one for the Generator and one for the Discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f09512",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f87c35",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "\n",
    "There are also two different types of losses that we will need.\n",
    "**Adversarial loss**\n",
    "This loss describes how well the discriminator can tell the difference between real and generated images.\n",
    "In our case, this will be a sort of classification loss - we will use Cross Entropy.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "The adversarial loss will be applied differently to the generator and the discriminator! Be very careful!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bc0f2",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "\n",
    "**Cycle/reconstruction loss**\n",
    "The cycle loss is there to make sure that the generator doesn't output an image that looks nothing like the input!\n",
    "Indeed, by training the generator to be able to cycle back to the original image, we are making sure that it makes a minimum number of changes.\n",
    "The cycle loss is applied only to the generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6f534",
   "metadata": {
    "tags": []
   },
   "source": [
    "Stuff about the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffcf76",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    mnist, batch_size=32, drop_last=True, shuffle=True\n",
    ")  # We will use the same dataset as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34391d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "TODO - Describe set_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da884ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def set_requires_grad(module, value=True):\n",
    "    \"\"\"Sets `requires_grad` on a `module`'s parameters to `value`\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51318090",
   "metadata": {
    "tags": []
   },
   "source": [
    "TODO - Describe EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def exponential_moving_average(model, ema_model, beta=0.999):\n",
    "    \"\"\"Update the EMA model's parameters with an exponential moving average\"\"\"\n",
    "    for param, ema_param in zip(model.parameters(), ema_model.parameters()):\n",
    "        ema_param.data.mul_(beta).add_((1 - beta) * param.data)\n",
    "\n",
    "\n",
    "def copy_parameters(source_model, target_model):\n",
    "    \"\"\"Copy the parameters of a model to another model\"\"\"\n",
    "    for param, target_param in zip(\n",
    "        source_model.parameters(), target_model.parameters()\n",
    "    ):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf872991",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_ema = Generator(deepcopy(unet), style_encoder=deepcopy(style_encoder))\n",
    "generator_ema = generator_ema.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15198f0",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-info\"><h4>Task 3.2: Training!</h4>\n",
    "\n",
    "TODO - the task is to choose where to apply set_requires_grad\n",
    "<ul>\n",
    "  <li>Choose the values for `set_requires_grad`. Hint: which part of the code is training the generator? Which part is training the discriminator</li>\n",
    "  <li>Choose the values of `set_requires_grad`, again. Hint: you may want to switch</li>\n",
    "  <li>Choose the sign of the discriminator loss. Hint: what does the discriminator want to do?</li>\n",
    "</ul>\n",
    "Let's train the StarGAN one batch a time.\n",
    "While you watch the model train, consider whether you think it will be successful at generating counterfactuals in the number of steps we give it. What is the minimum number of iterations you think are needed for this to work, and how much time do yo uthink it will take?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e51f7",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # This is a nice library for showing progress bars\n",
    "\n",
    "\n",
    "losses = {\"cycle\": [], \"adv\": [], \"disc\": []}\n",
    "for epoch in range(15):\n",
    "    for x, y in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # get the target y by shuffling the classes\n",
    "        # get the style sources by random sampling\n",
    "        random_index = torch.randperm(len(y))\n",
    "        x_style = x[random_index].clone()\n",
    "        y_target = y[random_index].clone()\n",
    "\n",
    "        set_requires_grad(generator, True)\n",
    "        set_requires_grad(discriminator, False)\n",
    "        optimizer_g.zero_grad()\n",
    "        # Get the fake image\n",
    "        x_fake = generator(x, x_style)\n",
    "        # Try to cycle back\n",
    "        x_cycled = generator(x_fake, x)\n",
    "        # Discriminate\n",
    "        discriminator_x_fake = discriminator(x_fake)\n",
    "        # Losses to  train the generator\n",
    "\n",
    "        # 1. make sure the image can be reconstructed\n",
    "        cycle_loss = cycle_loss_fn(x, x_cycled)\n",
    "        # 2. make sure the discriminator is fooled\n",
    "        adv_loss = adversarial_loss_fn(discriminator_x_fake, y_target)\n",
    "\n",
    "        # Optimize the generator\n",
    "        (cycle_loss + adv_loss).backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        set_requires_grad(generator, False)\n",
    "        set_requires_grad(discriminator, True)\n",
    "        optimizer_d.zero_grad()\n",
    "        #\n",
    "        discriminator_x = discriminator(x)\n",
    "        discriminator_x_fake = discriminator(x_fake.detach())\n",
    "        # Losses to train the discriminator\n",
    "        # 1. make sure the discriminator can tell real is real\n",
    "        real_loss = adversarial_loss_fn(discriminator_x, y)\n",
    "        # 2. make sure the discriminator can tell fake is fake\n",
    "        fake_loss = -adversarial_loss_fn(discriminator_x_fake, y_target)\n",
    "        #\n",
    "        disc_loss = (real_loss + fake_loss) * 0.5\n",
    "        disc_loss.backward()\n",
    "        # Optimize the discriminator\n",
    "        optimizer_d.step()\n",
    "\n",
    "        losses[\"cycle\"].append(cycle_loss.item())\n",
    "        losses[\"adv\"].append(adv_loss.item())\n",
    "        losses[\"disc\"].append(disc_loss.item())\n",
    "        exponential_moving_average(generator, generator_ema)\n",
    "    # Copy the EMA model's parameters to the generator\n",
    "    copy_parameters(generator_ema, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce33bf",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "...this time again. &#x1F682; &#x1F68B; &#x1F68B; &#x1F68B;\n",
    "\n",
    "Once training is complete, we can plot the losses to see how well the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[\"cycle\"], label=\"Cycle loss\")\n",
    "plt.plot(losses[\"adv\"], label=\"Adversarial loss\")\n",
    "plt.plot(losses[\"disc\"], label=\"Discriminator loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870558f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Questions</h3>\n",
    "<ul>\n",
    "<li> Do the losses look like what you expected? </li>\n",
    "<li> How do these losses differ from the losses you would expect from a classifier? </li>\n",
    "<li> Based only on the losses, do you think the model is doing well? </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132834e",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "We can also look at some examples of the images that the generator is creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd84f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "axs[0].imshow(x[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[1].imshow(x_style[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[2].imshow(x_fake[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[3].imshow(x_cycled[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72306c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feda07bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 3</h2>\n",
    "You've now learned the basics of what makes up a StarGAN, and details on how to perform adversarial training.\n",
    "The same method can be used to create a StarGAN with different basic elements.\n",
    "For example, you can change the archictecture of the generators, or of the discriminator to better fit your data in the future.\n",
    "\n",
    "You know the drill... let us know on the exercise chat when you have arrived here!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a38b5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4: Evaluating the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242818a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating counterfactuals\n",
    "\n",
    "The first thing that we want to do is make sure that our GAN is able to create counterfactual images.\n",
    "To do this, we have to create them, and then pass them through the classifier to see if they are classified correctly.\n",
    "\n",
    "First, let's get the test dataset, so we can evaluate the GAN on unseen data.\n",
    "Then, let's get four prototypical images from the dataset as style sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a45f5",
   "metadata": {
    "title": "Loading the test dataset"
   },
   "outputs": [],
   "source": [
    "test_mnist = ColoredMNIST(\"data\", download=True, train=False)\n",
    "prototypes = {}\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    options = np.where(test_mnist.targets == i)[0]\n",
    "    # Note that you can change the image index if you want to use a different prototype.\n",
    "    image_index = 0\n",
    "    x, y = test_mnist[options[image_index]]\n",
    "    prototypes[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a2185",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "Let's have a look at the prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc84587",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(prototypes[i].permute(1, 2, 0))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Prototype {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98a449",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we need to use these prototypes to create counterfactual images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e005c7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 4.1: Create counterfactuals</h3>\n",
    "In the below, we will store the counterfactual images in the `counterfactuals` array.\n",
    "\n",
    "<ul>\n",
    "<li> Create a counterfactual image for each of the prototypes. </li>\n",
    "<li> Classify the counterfactual image using the classifier. </li>\n",
    "<li> Store the source and target labels; which is which?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e3298",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "num_images = 1000\n",
    "random_test_mnist = torch.utils.data.Subset(\n",
    "    test_mnist, np.random.choice(len(test_mnist), num_images, replace=False)\n",
    ")\n",
    "counterfactuals = np.zeros((4, num_images, 3, 28, 28))\n",
    "\n",
    "predictions = []\n",
    "source_labels = []\n",
    "target_labels = []\n",
    "\n",
    "for i, (x, y) in tqdm(enumerate(random_test_mnist), total=num_images):\n",
    "    for lbl in range(4):\n",
    "        # Create the counterfactual\n",
    "        x_fake = generator(\n",
    "            x.unsqueeze(0).to(device), prototypes[lbl].unsqueeze(0).to(device)\n",
    "        )\n",
    "        # Predict the class of the counterfactual image\n",
    "        pred = model(x_fake)\n",
    "\n",
    "        # Store the source and target labels\n",
    "        source_labels.append(y)  # The original label of the image\n",
    "        target_labels.append(lbl)  # The desired label of the counterfactual image\n",
    "        # Store the counterfactual image and prediction\n",
    "        counterfactuals[lbl][i] = x_fake.cpu().detach().numpy()\n",
    "        predictions.append(pred.argmax().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc433ec",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "Let's plot the confusion matrix for the counterfactual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_cm = confusion_matrix(target_labels, predictions, normalize=\"true\")\n",
    "sns.heatmap(cf_cm, annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe438af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Questions</h3>\n",
    "<ul>\n",
    "<li> How well is our GAN doing at creating counterfactual images? </li>\n",
    "<li> Does your choice of prototypes matter? Why or why not? </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790e598",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's also plot some examples of the counterfactual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2306de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(range(num_images), 4):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    for j, ax in enumerate(axs):\n",
    "        ax.imshow(counterfactuals[j][i].transpose(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Class {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb80882",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Questions</h3>\n",
    "<ul>\n",
    "<li>Can you easily tell which of these images is the original, and which ones are the counterfactuals?</li>\n",
    "<li>What is your hypothesis for the features that define each class?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320f835",
   "metadata": {},
   "source": [
    "# Part 5: Highlighting Class-Relevant Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ffd8b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "At this point we have:\n",
    "- A classifier that can differentiate between image of different classes\n",
    "- A GAN that has correctly figured out how to change the class of an image\n",
    "\n",
    "Let's try putting the two together to see if we can figure out what exactly makes a class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b238fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = [random_test_mnist[i] for i in range(batch_size)]\n",
    "x = torch.stack([b[0] for b in batch])\n",
    "y = torch.tensor([b[1] for b in batch])\n",
    "x_fake = torch.tensor(counterfactuals[0, :batch_size])\n",
    "x = x.to(device).float()\n",
    "y = y.to(device)\n",
    "x_fake = x_fake.to(device).float()\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = integrated_gradients.attribute(x, baselines=x_fake, target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f24a63",
   "metadata": {
    "title": "Another visualization function"
   },
   "outputs": [],
   "source": [
    "def visualize_color_attribution_and_counterfactual(\n",
    "    attribution, original_image, counterfactual_image\n",
    "):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "    counterfactual_image = np.transpose(counterfactual_image, (1, 2, 0))\n",
    "\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax0.imshow(original_image)\n",
    "    ax0.set_title(\"Image\")\n",
    "    ax0.axis(\"off\")\n",
    "    ax1.imshow(counterfactual_image)\n",
    "    ax1.set_title(\"Counterfactual\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(np.abs(attribution))\n",
    "    ax2.set_title(\"Attribution\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059da2c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for idx in range(batch_size):\n",
    "    print(\"Source class:\", y[idx].item())\n",
    "    print(\"Target class:\", 0)\n",
    "    visualize_color_attribution_and_counterfactual(\n",
    "        attributions[idx].cpu().numpy(), x[idx].cpu().numpy(), x_fake[idx].cpu().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66d7b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Questions</h3>\n",
    "<ul>\n",
    "<li> Do the attributions explain the differences between the images and their counterfactuals? </li>\n",
    "<li> What happens when the \"counterfactual\" and the original image are of the same class? Why do you think this is? </li>\n",
    "<li> Do you have a more refined hypothesis for what makes each class unique? </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c66f3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 4</h2>\n",
    "At this point you have:\n",
    "- Created a StarGAN that can change the class of an image\n",
    "- Evaluated the StarGAN on unseen data\n",
    "- Used the StarGAN to create counterfactual images\n",
    "- Used the counterfactual images to highlight the differences between classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2462b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 6: Exploring the Style Space, finding the answer\n",
    "By now you will have hopefully noticed that it isn't the exact color of the image that determines its class, but that two images with a very similar color can be of different classes!\n",
    "\n",
    "Here is an example of two images that are very similar in color, but are of different classes.\n",
    "![same_color_diff_class](assets/same_color_diff_class.png)\n",
    "While both of the images are yellow, the attribution tells us (if you squint!) that one of the yellows has slightly more blue in it!\n",
    "\n",
    "Conversely, here is an example of two images with very different colors, but that are of the same class:\n",
    "![same_class_diff_color](assets/same_class_diff_color.png)\n",
    "Here the attribution is empty! Using the discriminative attribution we can see that the significant color change doesn't matter at all!\n",
    "\n",
    "\n",
    "So color is important... but not always? What's going on!?\n",
    "There is a final piece of information that we can use to solve the puzzle: the style space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <div class=\"alert alert-block alert-info\"><h3>Task 6.1: Explore the style space</h3>\n",
    "# Let's take a look at the style space.\n",
    "# We will use the style encoder to encode the style of the images and then use PCA to visualize it.\n",
    "# </div>\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adb8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = []\n",
    "labels = []\n",
    "for img, label in random_test_mnist:\n",
    "    styles.append(\n",
    "        style_encoder(img.unsqueeze(0).to(device)).cpu().detach().numpy().squeeze()\n",
    "    )\n",
    "    labels.append(label)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "styles_pca = pca.fit_transform(styles)\n",
    "\n",
    "# Plot the PCA\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(4):\n",
    "    plt.scatter(\n",
    "        styles_pca[np.array(labels) == i, 0],\n",
    "        styles_pca[np.array(labels) == i, 1],\n",
    "        label=f\"Class {i}\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56f705",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 6.2: Adding color to the style space</h3>\n",
    "We know that color is important. Does interpreting the style space as colors help us understand better?\n",
    "\n",
    "Let's use the style space to color the PCA plot.\n",
    "</div>\n",
    "TODO WIP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd14d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going Further\n",
    "\n",
    "Here are some ideas for how to continue with this notebook:\n",
    "\n",
    "1. Improve the classifier. This code uses a VGG network for the classification. On the synapse dataset, we will get a validation accuracy of around 80%. Try to see if you can improve the classifier accuracy.\n",
    "    * (easy) Data augmentation: The training code for the classifier is quite simple in this example. Enlarge the amount of available training data by adding augmentations (transpose and mirror the images, add noise, change the intensity, etc.).\n",
    "    * (easy) Network architecture: The VGG network has a few parameters that one can tune. Try a few to see what difference it makes.\n",
    "    * (easy) Inspect the classifier predictions: Take random samples from the test dataset and classify them. Show the images together with their predicted and actual labels.\n",
    "    * (medium) Other networks:  Try different architectures (e.g., a [ResNet](https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/#resnet-from-scratch)) and see if the accuracy can be improved.\n",
    "\n",
    "2. Explore the CycleGAN.\n",
    "    * (easy) The example code below shows how to translate between GABA and acetylcholine. Try different combinations. Can you start to see differences between some pairs of classes? Which are the ones where the differences are the most or the least obvious? Can you see any differences that aren't well described by the mask? How would you describe these?\n",
    "\n",
    "3. Try on your own data!\n",
    "    * Have a look at how the synapse images are organized in `data/raw/synapses`. Copy the directory structure and use your own images. Depending on your data, you might have to adjust the image size (128x128 for the synapses) and number of channels in the VGG network and CycleGAN code."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
