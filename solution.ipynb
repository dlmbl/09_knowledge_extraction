{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdd8648",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Exercise 9: Knowledge Extraction from a Convolutional Neural Network\n",
    "\n",
    "In the following exercise we will train a convolutional neural network to classify electron microscopy images of Drosophila synapses, based on which neurotransmitter they contain. We will then train a CycleGAN and use a method called Discriminative Attribution from Counterfactuals (DAC) to understand how the network performs its classification, effectively going back from prediction to image data.\n",
    "\n",
    "![synister.png](attachment:synister.png)\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook was written by Jan Funke and modified by Tri Nguyen and Diane Adjavon, using code from Nils Eckstein and a modified version of the [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cd6a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>knowledge-extraction</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723baf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Start here (AKA checkpoint 0)</h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c3a46",
   "metadata": {},
   "source": [
    "## Part 1: Image Classification\n",
    "\n",
    "### Exercise 1: Train an Image Classifier\n",
    "\n",
    "In this section, we will implement and train a VGG classifier to classify images of synapses into one of six classes, corresponding to the neurotransmitter type that is released at the synapse: GABA, acethylcholine, glutamate, octopamine, serotonin, and dopamine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105ba2a",
   "metadata": {},
   "source": [
    "\n",
    "The data we use for this exercise is located in `data/raw/synapses`, where we have one subdirectory for each neurotransmitter type. Look at a few examples to familiarize yourself with the dataset. You will notice that the dataset is not balanced, i.e., we have much more examples of one class versus another one.\n",
    "\n",
    "This class imbalance is problematic for training a classifier. Imagine that 99% of our images are of one class, then the classifier would do really well predicting this class all the time, without having learnt anything of substance. It is therefore important to balance the dataset, i.e., present the same number of images per class to the classifier during training.\n",
    "\n",
    "First, we split the available images into a train, validation, and test dataset with proportions of 0.7, 0.15, and 0.15, respectively. Each image should be returned as a 2D `numpy` array with float values between 0 and 1. The label for each image should be the name of the directory for this class (e.g., `0_gaba`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6c153",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create a dataset for all images of all classes\n",
    "full_dataset = ImageFolder(root=\"data/raw/synapses\", transform=transform)\n",
    "\n",
    "# Rename the classes\n",
    "full_dataset.classes = [x.split(\"_\")[-1] for x in full_dataset.classes]\n",
    "class_to_idx = {x.split(\"_\")[-1]: v for x, v in full_dataset.class_to_idx.items()}\n",
    "full_dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# randomly split the dataset into train, validation, and test\n",
    "num_images = len(full_dataset)\n",
    "# ~70% for training\n",
    "num_training = int(0.7 * num_images)\n",
    "# ~15% for validation\n",
    "num_validation = int(0.15 * num_images)\n",
    "# ~15% for testing\n",
    "num_test = num_images - (num_training + num_validation)\n",
    "# split the data randomly (but with a fixed random seed)\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [num_training, num_validation, num_test],\n",
    "    generator=torch.Generator().manual_seed(23061912),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514abfc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 1.1: Create a Balanced Dataloader</h4>\n",
    "\n",
    "Below define a `sampler` that samples images of classes with skewed probabilities to account for the different number of items in each class.\n",
    "\n",
    "The sampler\n",
    "- Counts the number of samples in each class\n",
    "- Gets the weight-per-label as an inverse of the frequency\n",
    "- Get the weight-per-sample\n",
    "- Create a `WeightedRandomSampler` based on these weights\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights in training dataset for balanced sampling\n",
    "def balanced_sampler(dataset):\n",
    "    # Get a list of targets from the dataset\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        # A Subset is a specific type of dataset, which does not directly have access to the targets.\n",
    "        targets = torch.tensor(dataset.dataset.targets)[dataset.indices]\n",
    "    else:\n",
    "        targets = dataset.targets\n",
    "\n",
    "    counts = torch.bincount(targets)  # Count the number of samples for each class\n",
    "    label_weights = (\n",
    "        1.0 / counts\n",
    "    )  # Get the weight-per-label as an inverse of the frequency\n",
    "    weights = label_weights[targets]  # Get the weight-per-sample\n",
    "\n",
    "    # Optional: Print the Counts and Weights to make sure lower frequency classes have higher weights\n",
    "    print(\"Number of images per class:\")\n",
    "    for c, n, w in zip(full_dataset.classes, counts, label_weights):\n",
    "        print(f\"\\t{c}:\\tn={n}\\tweight={w}\")\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights, len(weights)\n",
    "    )  # Create a sampler based on these weights\n",
    "    return sampler\n",
    "\n",
    "\n",
    "sampler = balanced_sampler(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4944d",
   "metadata": {},
   "source": [
    "We make a `torch` `DataLoader` that takes our `sampler` to create batches of eight images and their corresponding labels.\n",
    "Each image should be randomly and equally selected from the six available classes (i.e., for each image sample pick a random class, then pick a random image from this class).\n",
    "\n",
    "We additionally create a validation data loader and a test data loader.\n",
    "These do not need to be sampled in a special way, and can load more images at once because the evaluation pass is less memory intensive than the training pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data loader will serve 8 images in a \"mini-batch\" at a time\n",
    "dataloader = DataLoader(train_dataset, batch_size=8, drop_last=True, sampler=sampler)\n",
    "# These data laoders serve 32 images in a \"mini-batch\"\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(validation_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0b683",
   "metadata": {},
   "source": [
    "The cell below visualizes a single, randomly chosen batch from the training data loader. Feel free to execute this cell multiple times to get a feeling for the dataset and that your sampler gives batches of evenly distributed synapse types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4c4b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_batch(x, y):\n",
    "    fig, axs = plt.subplots(1, x.shape[0], figsize=(14, 14), sharey=True)\n",
    "    for i in range(x.shape[0]):\n",
    "        axs[i].imshow(np.squeeze(x[i]), cmap=\"gray\")\n",
    "        axs[i].set_title(train_dataset.dataset.classes[y[i].item()])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# show a random batch from the data loader\n",
    "# (run this cell repeatedly to see different batches)\n",
    "for x, y in dataloader:\n",
    "    show_batch(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc5177",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 1.2: Create a VGG Network, Loss, and Optimizer</h4>\n",
    "\n",
    "We will use a VGG network to classify the synapse images. The input to the network will be a 2D image as provided by your dataloader. The output will be a vector of six floats, corresponding to the probability of the input to belong to the six classes.\n",
    "\n",
    "Implement a VGG network with the following specificatons:\n",
    "\n",
    "* the constructor takes the size of the 2D input image as height and width\n",
    "* the network starts with a downsample path consisting of:\n",
    "    * one convolutional layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.Conv2d` layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.MaxPool2d` with a `downsample_factor` of (2, 2) at each level\n",
    "* followed by three more downsampling paths like the one above, every time doubling the number of `fmaps` (i.e., the second one will have 24, the third 48, and the fourth 96). Make sure to keep track of the `current_fmaps` each time!\n",
    "* then two times:\n",
    "    * `nn.Linear` layer with `out_features=4096`. Be careful withe in `in_features` of the first one, which will depend on the size of the previous output!\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.DropOut`\n",
    "* Finally, one more fully connected layer with\n",
    "    * `nn.Linear` to the 6 classes\n",
    "    * no activation function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg2D(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        fmaps=12,\n",
    "        downsample_factors=[(2, 2), (2, 2), (2, 2), (2, 2)],\n",
    "        output_classes=6,\n",
    "    ):\n",
    "        super(Vgg2D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        current_fmaps, h, w = tuple(input_size)\n",
    "        current_size = (h, w)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "            features += [\n",
    "                torch.nn.Conv2d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(fmaps, fmaps, kernel_size=3, padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool2d(downsample_factors[i]),\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c / d) for c, d in zip(current_size, downsample_factors[i])\n",
    "            )\n",
    "            check = (\n",
    "                s * d == c for s, d, c in zip(size, downsample_factors[i], current_size)\n",
    "            )\n",
    "            assert all(check), \"Can not downsample %s by chosen downsample factor\" % (\n",
    "                current_size,\n",
    "            )\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] * current_size[1] * current_fmaps, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, output_classes),\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "    def forward(self, raw):\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "\n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "\n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4081f55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 1.3: Train the VGG Network</h4>\n",
    "\n",
    "We'll start by creating the VGG with the default parameters and push it to a GPU if there is one available. Then we'll define the training loss and optimizer.\n",
    "The training and evaluation loops have been defined for you, so after that just train your network!\n",
    "\n",
    "TODO:\n",
    "- Choose a loss\n",
    "- Create an Adam optimizer and set its learning rate\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of our images\n",
    "for x, y in train_dataset:\n",
    "    input_size = x.shape\n",
    "    break\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg2D(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a GPU, if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee99688",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c384c1e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Solution to Exercise 1.3 #\n",
    "############################\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc9b93",
   "metadata": {},
   "source": [
    "The next cell defines some convenience functions for training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6108080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "\n",
    "    # set the model into train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    for x, y in tqdm(dataloader, \"train\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += l\n",
    "        num_batches += 1\n",
    "\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(dataloader, name=\"\"):\n",
    "    # Set the model to train mode\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        predictions = torch.argmax(probs, dim=1)\n",
    "\n",
    "        correct += int(torch.sum(predictions == y).cpu().detach().numpy())\n",
    "        total += len(y)\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651863a1",
   "metadata": {},
   "source": [
    "We are ready to train. After each epoch (roughly going through each training image once), we report the training loss and the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    epoch_loss = train()\n",
    "    print(f\"Epoch {epoch}, training loss={epoch_loss}\")\n",
    "\n",
    "    accuracy = evaluate(validation_dataloader, \"Validation\")\n",
    "    print(f\"Epoch {epoch}, validation accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972af7aa",
   "metadata": {},
   "source": [
    "Let's watch your model train!\n",
    "\n",
    "![model_train.jpeg](attachment:model_train.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee332d2",
   "metadata": {},
   "source": [
    "And now, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(test_dataloader, \"Testing\")\n",
    "print(f\"Final test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f3fd1",
   "metadata": {},
   "source": [
    "If you're unhappy with the accuracy above (which you should be...) we pre-trained a model for you for many more epochs. You can load it with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c142b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change this to True and run this cell if you want a shortcut\n",
    "yes_I_want_the_pretrained_model = True\n",
    "\n",
    "if yes_I_want_the_pretrained_model:\n",
    "    checkpoint = torch.load(\n",
    "        \"checkpoints/synapses/classifier/vgg_checkpoint\", map_location=device\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a219d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 1.4: Construct a confusion matrix</h4>\n",
    "\n",
    "We now have a classifier that can discriminate between images of different types. If you used the images we provided, the classifier is not perfect (you should get an accuracy of around 80%), but pretty good considering that there are six different types of images.\n",
    "\n",
    "To understand the performance of the classifier beyond a single accuracy number, we should build a confusion matrix that can more elucidate which classes are more/less misclassified and which classes are those wrong predictions confused with. You should get something like this:\n",
    "\n",
    "TODO\n",
    "Modify the `evaluation_cm` so that it returns a paired list of predicted class vs ground truth to produce a confusion matrix. You'll need to do the following steps.\n",
    "- Get the model output for the batch of data `(x, y)`\n",
    "- Turn the model output into a probability\n",
    "- Get the class predictions from the probabilities\n",
    "- Add the class predictions to a list of all predictions\n",
    "- Add the ground truths to a list of all ground truths\n",
    "\n",
    "</div>\n",
    "\n",
    "![confusion_matrix.png](attachment:256a54ec-d4cd-4952-84ca-00ebf0652b4e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87bc72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee775347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: return a paired list of predicted class vs ground-truth to produce a confusion matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_cm(dataloader, name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Get the model output\n",
    "        # Turn the model output into a probability\n",
    "        # Get the class predictions from the probabilities\n",
    "\n",
    "        predictions.extend(...)  # TODO add predictions to the list\n",
    "        ground_truths.extend(...)  # TODO add ground truths to the list\n",
    "    return (predictions, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129c213",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Solution for Exercise 1.4 #\n",
    "#########################\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_cm(dataloader, name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Get the model output\n",
    "        logits = model(x)\n",
    "        # Turn the model output into a probability\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        # Get the class predictions from the probabilities\n",
    "        batch_predictions = torch.argmax(probs, dim=1)\n",
    "\n",
    "        # append predictions and groundtruth to our big list,\n",
    "        # converting `tensor` objects to simple values through .item()\n",
    "        predictions.extend([k.item() for k in batch_predictions])\n",
    "        ground_truths.extend([k.item() for k in y])\n",
    "\n",
    "    return (predictions, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4032d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, groundtruth = evaluate_cm(test_dataloader, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921fbcc",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "# orginally from Runqi Yang;\n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, names, labels=None, title=None, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "      confusion_matrix: np.array\n",
    "      labels: list\n",
    "          List of integer values to determine which classes to consider.\n",
    "      names:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if labels is not None:\n",
    "        assert len(names) == len(labels)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = \"%.1f%%\\n%d/%d\" % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = \"\"\n",
    "            else:\n",
    "                annot[i, j] = \"%.1f%%\\n%d\" % (p, c)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = sns.heatmap(\n",
    "        cm_perc, annot=annot, fmt=\"\", vmax=100, xticklabels=names, yticklabels=names\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "names = [\"gaba\", \"acetylcholine\", \"glutamate\", \"serotonine\", \"octopamine\", \"dopamine\"]\n",
    "cm_analysis(prediction, groundtruth, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db372459",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-banner alert-info\">\n",
    "<h4>Questions</h4>\n",
    "- What observations can we make from the confusion matrix?\n",
    "- Does the classifier do better on some synapse classes than other?\n",
    "- If you have time later, which ideas would you try to train a better predictor?\n",
    "\n",
    "Let us know your thoughts on Element.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85422166",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 1</h1>\n",
    "\n",
    "We now have:\n",
    "- A classifier that is pretty good a predicting neurotransmitters from EM images.\n",
    "\n",
    "This is surprising, since we could not (yet) have made these predictions manually! If you're skeptical, feel free to explore the data a bit more and see for yourself if you can tell the difference betwee, say, GABAergic and glutamatergic synapses.\n",
    "\n",
    "So this is an interesting situation: The VGG network knows something we don't quite know. In the next section, we will see how we can find and then visualize the relevant differences between images of different types.\n",
    "\n",
    "This concludes the first section. Let us know on Element if you have arrived here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a05654",
   "metadata": {},
   "source": [
    "## Part 2: Masking the relevant part of the image\n",
    "\n",
    "In this section we will make a first attempt at highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afb183",
   "metadata": {},
   "source": [
    "### Exercise 2: Attributions through integrated gradients\n",
    "\n",
    "Attribution is the process of finding out, based on the output of a neural network, which pixels in the input are (most) responsible. Another way of thinking about it is: which pixels would need to change in order for the network's output to change.\n",
    "\n",
    "Here we will look at an example of an attribution method called [Integrated Gradients](https://captum.ai/docs/extension/integrated_gradients). If you have a bit of time, have a look at this [super fun exploration of attribution methods](https://distill.pub/2020/attribution-baselines/), especially the explanations on Integrated Gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef4868",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 2.1 Get a normalized attribution</h4>\n",
    "\n",
    "In this next part, we will get attributions on single batch. We use a library called [captum](https://captum.ai), and focus on the `IntegratedGradients` method.\n",
    "Create an `IntegratedGradients` object and run attribution on `x,y` obtained above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2373741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "############### Exercise 2.1 TODO ############\n",
    "# Create an integrated gradients object.\n",
    "integrated_gradients = ...\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1257a",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Solution for Exercise 2.1 #\n",
    "#########################\n",
    "\n",
    "# Create an integrated gradients object.\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = integrated_gradients.attribute(x, target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = (\n",
    "    attributions.cpu().numpy()\n",
    ")  # Move the attributions from the GPU to the CPU, and turn then into numpy arrays for future processing\n",
    "attributions = np.array(\n",
    "    [a / np.max(np.abs(a)) for a in attributions]\n",
    ")  # Normalize the attributions between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f532e",
   "metadata": {},
   "source": [
    "Here is an example for an image, and its corresponding attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41794eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(x.cpu()[0].squeeze(), cmap=\"gray\")\n",
    "ax2.imshow(attributions[0].squeeze(), cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b140ca",
   "metadata": {},
   "source": [
    "### Smoothing the attribution into a mask\n",
    "\n",
    "The attributions that we see are grainy and difficult to interpret because they are a pixel-wise attribution. We apply some smoothing and thresholding on the attributions so that they represent region masks rather than pixel masks. The following code is runnable with no modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ba4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import copy\n",
    "\n",
    "struc = 10\n",
    "sigma = 11\n",
    "\n",
    "\n",
    "def get_mask(attrs, threshold=0.5):\n",
    "    # Threshold to turn into a mask\n",
    "    mask = np.array(np.any(attrs > threshold, axis=0), dtype=np.uint8)\n",
    "    # morphological closing and Gaussian Blur\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (struc, struc))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask_cp = copy.deepcopy(mask)\n",
    "    mask_weight = cv2.GaussianBlur(mask_cp.astype(float), (sigma, sigma), 0)\n",
    "    return mask_weight\n",
    "\n",
    "\n",
    "def plot_mask(x, mask, y=None, baseline=None):\n",
    "    nan_mask = mask\n",
    "    nan_mask[nan_mask == 0] = np.nan\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(x, cmap=\"gray\")\n",
    "    ax.imshow(nan_mask, cmap=\"coolwarm\", vmin=0, vmax=1, alpha=0.5)\n",
    "    ax.axis(\"off\")\n",
    "    if y is not None:\n",
    "        ax.set_title(train_dataset.dataset.classes[y.item()])\n",
    "    return\n",
    "\n",
    "\n",
    "def interactive_attribution(idx=0, threshold=0.5):\n",
    "    image = x[idx].unsqueeze(0).cpu()\n",
    "    target = y[idx].unsqueeze(0)\n",
    "    attrs = attributions[idx]\n",
    "    mask_weight = get_mask(attrs, threshold=threshold)\n",
    "    plot_mask(image.squeeze(), mask_weight.squeeze(), y=target)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777a56f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 2.2 Visualizing the results</h4>\n",
    "\n",
    "The code above creates a small widget to interact with the results of this analysis. You'll see in the masking code that there is a free parameter called \"threshold\". This threshold affects the size of the mask. If you modify the threshold below, you will see that different objects appear and disappear.\n",
    "You can also look at different images by sliding over the index.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efbf9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-banner alert-info\">\n",
    "<h4>Questions</h4>\n",
    "- Are there some recognisable objects or parts of the synapse that show up in several examples?\n",
    "- Are there some objects that seem secondary because they only appear at certain threshold levels?\n",
    "\n",
    "Tell us what you see on Element!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee0b5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "interact(\n",
    "    interactive_attribution,\n",
    "    idx=(0, dataloader.batch_size - 1),\n",
    "    threshold=(0.0, 1.0, 0.05),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732375b",
   "metadata": {},
   "source": [
    "### Bonus exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c83f1e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>BONUS Exercise 2.3: changing the baseline</h4>\n",
    "\n",
    "Many existing attribution algorithms are comparative: they show which pixels of the input are responsible for a network output *compared to a baseline*.\n",
    "The baseline is often set to an all 0 tensor, but the choice of the baseline affects the output.\n",
    "(For an interactive illustration of how the baseline affects the output, see [this Distill paper](https://distill.pub/2020/attribution-baselines/))\n",
    "\n",
    "You can change the baseline used by the `integrated_gradients` object.\n",
    "\n",
    "Use the command:\n",
    "```\n",
    "?integrated_gradients.attribute\n",
    "```\n",
    "To get more details about how to include the baseline.\n",
    "\n",
    "Try using the code above to change the baseline and see how this affects the output. Do different features get highlighted?\n",
    "For example you can try:\n",
    "- Random noise as a baseline\n",
    "- Another image from the dataset\n",
    "- A blurred/noisy version of the original image as a baseline.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f558999",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>BONUS Exercise  2.4: Using different attributions.</h4>\n",
    "\n",
    "\n",
    "\n",
    "[`captum`](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) has access to various different attribution algorithms.\n",
    "\n",
    "Replace `IntegratedGradients` with different attribution methods. Are they consistent with each other?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec96ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 2</h1>\n",
    "Let us know on Element when you've reached this point!\n",
    "\n",
    "At this point we have:\n",
    "<ol>\n",
    "    <li>Trained a classifier that can predict neurotransmitters from EM-slices of synapses.</li>\n",
    "    <li>Found a way to mask the parts of the image that seem to be relevant for the classification, using integrated gradients.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c1b73a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 3: Train a GAN to Translate Images\n",
    "\n",
    "To gain insight into how the trained network classify images, we will use [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412), a feature attribution with counterfactual explanations methodology. This method employs a so-called CycleGAN to translate images from one class to another to make counterfactual explanations.\n",
    "\n",
    "In this example, we will train a CycleGAN network that translates GABAergic synapses to acetylcholine synapses (you can also train other pairs too by changing the classes below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f72fb5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def class_dir(name):\n",
    "    return f\"{class_to_idx[name]}_{name}\"\n",
    "\n",
    "\n",
    "classes = [\"gaba\", \"acetylcholine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9472cc",
   "metadata": {},
   "source": [
    "### Exercise 3: Train a GAN\n",
    "\n",
    "Yes, really!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67d39e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Creating a specialized dataset</h4>\n",
    "\n",
    "The CycleGAN works on only 2 classes at a time, but our full dataset has 6. Below, we will use the `Subset` dataset from `torch.utils.data` to get the data from these two classes.\n",
    "\n",
    "A `Subset` is created as follows:\n",
    "```\n",
    "subset = Subset(dataset, indices)\n",
    "```\n",
    "\n",
    "And the chosen indices can be obtained again using `subset.indices`.\n",
    "\n",
    "Run the cell below to generate the datasets:\n",
    "- `gan_train_dataset`\n",
    "- `gan_test_dataset`\n",
    "- `gan_val_dataset`\n",
    "\n",
    "We will use them below to train the CycleGAN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to get a subset of classes\n",
    "def get_indices(dataset, classes):\n",
    "    \"\"\"Get the indices of elements of classA and classB in the dataset.\"\"\"\n",
    "    indices = []\n",
    "    for cl in classes:\n",
    "        indices.append(torch.tensor(dataset.targets) == class_to_idx[cl])\n",
    "    logical_or = sum(indices) > 0\n",
    "    return torch.where(logical_or)[0]\n",
    "\n",
    "\n",
    "def set_intersection(a_indices, b_indices):\n",
    "    \"\"\"Get intersection of two sets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_indices: torch.Tensor\n",
    "    b_indices: torch.Tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    intersection: torch.Tensor\n",
    "        The elements contained in both a_indices and b_indices.\n",
    "    \"\"\"\n",
    "    a_cat_b, counts = torch.cat([a_indices, b_indices]).unique(return_counts=True)\n",
    "    intersection = a_cat_b[torch.where(counts.gt(1))]\n",
    "    return intersection\n",
    "\n",
    "\n",
    "# Getting training, testing, and validation indices\n",
    "gan_idx = get_indices(full_dataset, classes)\n",
    "\n",
    "gan_train_idx = set_intersection(torch.tensor(train_dataset.indices), gan_idx)\n",
    "gan_test_idx = set_intersection(torch.tensor(test_dataset.indices), gan_idx)\n",
    "gan_val_idx = set_intersection(torch.tensor(validation_dataset.indices), gan_idx)\n",
    "\n",
    "# Checking that the subsets are complete\n",
    "assert len(gan_train_idx) + len(gan_test_idx) + len(gan_val_idx) == len(gan_idx)\n",
    "\n",
    "# Generate three datasets based on the above indices.\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "gan_train_dataset = Subset(full_dataset, gan_train_idx)\n",
    "gan_test_dataset = Subset(full_dataset, gan_test_idx)\n",
    "gan_val_dataset = Subset(full_dataset, gan_val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224825e",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "![cycle.png](attachment:79141d1d-6c9e-41ed-a396-0fd5fe7952c8.png)\n",
    "\n",
    "In the following, we create a [CycleGAN model](https://arxiv.org/pdf/1703.10593.pdf). It is a Generative Adversarial model that is trained to turn one class of images X (for us, GABA) into a different class of images Y (for us, Acetylcholine).\n",
    "\n",
    "It has two generators:\n",
    "   - Generator G takes a GABA image and tries to turn it into an image of an Acetylcholine synapse. When given an image that is already showing an Acetylcholine synapse, G should just re-create the same image: these are the `identities`.\n",
    "   - Generator F takes a Acetylcholine image and tries to turn it into an image of an GABA synapse. When given an image that is already showing a GABA synapse, F should just re-create the same image: these are the `identities`.\n",
    "\n",
    "\n",
    "When in training mode, the CycleGAN will also create a `reconstruction`. These are images that are passed through both generators.\n",
    "For example, a GABA image will first be transformed by G to Acetylcholine, then F will turn it back into GABA.\n",
    "This is achieved by training the network with a cycle-consistency loss. In our example, this is an L2 loss between the `real` GABA image and the `reconstruction` GABA image.\n",
    "\n",
    "But how do we force the generators to change the class of the input image? We use a discriminator for each.\n",
    "   - DX tries to recognize fake GABA images: F will need to create images realistic and GABAergic enough to trick it.\n",
    "   - DY tries to recognize fake Acetylcholine images: G will need to create images realistic and cholinergic enough to trick it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459dba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import functools\n",
    "from cycle_gan.models.networks import ResnetGenerator, NLayerDiscriminator, GANLoss\n",
    "\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    \"\"\"Cycle GAN\n",
    "\n",
    "    Has:\n",
    "    - Two class names\n",
    "    - Two Generators\n",
    "    - Two Discriminators\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, class1, class2, input_nc=1, output_nc=1, ngf=64, ndf=64, use_dropout=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        class1: str\n",
    "            Label of the first class\n",
    "        class2: str\n",
    "            Label of the second class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        norm_layer = functools.partial(\n",
    "            nn.InstanceNorm2d, affine=False, track_running_stats=False\n",
    "        )\n",
    "        self.classes = [class1, class2]\n",
    "        self.inverse_keys = {\n",
    "            class1: class2,\n",
    "            class2: class1,\n",
    "        }  # i.e. what is the other key?\n",
    "        self.generators = nn.ModuleDict(\n",
    "            {\n",
    "                classname: ResnetGenerator(\n",
    "                    input_nc,\n",
    "                    output_nc,\n",
    "                    ngf,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_dropout=use_dropout,\n",
    "                    n_blocks=9,\n",
    "                )\n",
    "                for classname in self.classes\n",
    "            }\n",
    "        )\n",
    "        self.discriminators = nn.ModuleDict(\n",
    "            {\n",
    "                classname: NLayerDiscriminator(\n",
    "                    input_nc, ndf, n_layers=3, norm_layer=norm_layer\n",
    "                )\n",
    "                for classname in self.classes\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        \"\"\"Creates fakes from the reals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict\n",
    "            classname -> images\n",
    "        train: boolean\n",
    "            If false, only the counterfactuals are generated and returned.\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fakes: dict\n",
    "            classname -> images of counterfactual images\n",
    "        identities: dict\n",
    "            classname -> images of images passed through their corresponding generator, if train is True\n",
    "            For example, images of class1 are passed through the generator that creates class1.\n",
    "            These should be identical to the input.\n",
    "            Not returned if `train` is `False`\n",
    "        reconstructions\n",
    "            classname -> images of reconstructed images (full cycle), if train is True.\n",
    "            Not returned if `train` is `False`\n",
    "        \"\"\"\n",
    "        fakes = {}\n",
    "        identities = {}\n",
    "        reconstructions = {}\n",
    "        for k, batch in x.items():\n",
    "            inv_k = self.inverse_keys[k]\n",
    "            # Counterfactual: class changes\n",
    "            fakes[inv_k] = self.generators[inv_k](batch)\n",
    "            if train:\n",
    "                # From counterfactual back to original, class changes again\n",
    "                reconstructions[k] = self.generators[k](fakes[inv_k])\n",
    "                # Identites: class does not change\n",
    "                identities[k] = self.generators[k](batch)\n",
    "        if train:\n",
    "            return fakes, identities, reconstructions\n",
    "        return fakes\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        \"\"\"Get discriminator opinion on x\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict\n",
    "            classname -> images\n",
    "        \"\"\"\n",
    "        discrimination = {}\n",
    "        for k, batch in x.items():\n",
    "            discrimination[k] = self.discriminators[k](batch)\n",
    "        return discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a750226",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cyclegan = CycleGAN(*classes)\n",
    "cyclegan.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd99a37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "You will notice above that the `CycleGAN` takes an input in the form of a dictionary, but our datasets and data-loaders return the data in the form of two tensors. Below are two utility functions that will swap from data from one to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to go to/from dictionaries/x,y tensors\n",
    "def get_as_xy(dictionary):\n",
    "    x = torch.cat([arr for arr in dictionary.values()])\n",
    "    y = []\n",
    "    for k, v in dictionary.items():\n",
    "        val = class_labels[k]\n",
    "        y += [\n",
    "            val,\n",
    "        ] * len(v)\n",
    "    y = torch.Tensor(y).to(x.device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_as_dictionary(x, y):\n",
    "    dictionary = {}\n",
    "    for k in classes:\n",
    "        val = class_to_idx[k]\n",
    "        # Get all of the indices for this class\n",
    "        this_class_indices = torch.where(y == val)\n",
    "        dictionary[k] = x[this_class_indices]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac104cd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 3.1: Creating the training loop</h4>\n",
    "\n",
    "Now that we have a model, our next task is to create a training loop for the CycleGAN. This is a bit more difficult than the training loop for our classifier.\n",
    "\n",
    "Here are some of the things to keep in mind during this exercise.\n",
    "\n",
    "1. The CycleGAN is (obviously) a GAN: a Generative Adversarial Network. What makes an adversarial network \"adversarial\" is that two different networks are working against each other. The loss that is used to optimize this is in our exercise `criterionGAN`. Although the specifics of this loss is beyond the score of this notebook, the idea is simple: the `criterionGAN` compares the output of the discriminator to a boolean-valued target. If we want the discriminator to think that it has seen a real image, we set the target to `True`. If we want the  discriminator to think that it has seen a generated image, we set the target to `False`. Note that it isn't important here whether the image *is* real, but **whether we want the discriminator to think it is real at that point**. (This will be important very soon 😉)\n",
    "\n",
    "2. Since the two networks are fighting each other, it is important to make sure that neither of them can cheat with information espionage. The CycleGAN implementation below is a turn-by-turn fight: we train the generator(s) and the discriminator(s) in alternating steps. When a model is not training, we will restrict its access to information by using `set_requries_grad` to `False`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycle_gan.util.image_pool import ImagePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b1cf1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "criterionIdt = nn.L1Loss()\n",
    "criterionCycle = nn.L1Loss()\n",
    "criterionGAN = GANLoss(\"lsgan\")\n",
    "criterionGAN.to(device)\n",
    "\n",
    "lambda_idt = 1\n",
    "pool_size = 32\n",
    "\n",
    "lambdas = {k: 1 for k in classes}\n",
    "image_pools = {classname: ImagePool(pool_size) for classname in classes}\n",
    "\n",
    "optimizer_g = torch.optim.Adam(cyclegan.generators.parameters(), lr=1e-4)\n",
    "optimizer_d = torch.optim.Adam(cyclegan.discriminators.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877e82c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>TODO</h4>\n",
    "\n",
    "In the code below, there are several spots with multiple options. Choose from among these, and delete or comment out the incorrect option.\n",
    "1. In `generator_step`: Choose whether the target to the`criterionGAN` should be `True` or `False`.\n",
    "2. In `discriminator_step`: Choose the target to the `criterionGAN` (note that there are two this time, one for the real images and one for the generated images)\n",
    "3. In `train_gan`: `set_requires_grad` correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54937beb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def set_requires_grad(module, value=True):\n",
    "    \"\"\"Sets `requires_grad` on a `module`'s parameters to `value`\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = value\n",
    "\n",
    "\n",
    "def generator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for generators G_X and G_Y\"\"\"\n",
    "    # Get all generated images\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    # Get discriminator opinion\n",
    "    discrimination = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        # Identity loss\n",
    "        # G_A should be identity if real_B is fed: ||G_A(B) - B||\n",
    "        loss_idt = criterionIdt(identities[k], reals[k]) * lambdas[k] * lambda_idt\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        #################### TODO Choice 1 #####################\n",
    "        # OPTION 1\n",
    "        # loss_G = criterionGAN(discrimination[k], False)\n",
    "        # OPTION2\n",
    "        # loss_G = criterionGAN(discrimination[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        # Forward cycle loss || G_B(G_A(A)) - A||\n",
    "        loss_cycle = criterionCycle(reconstructions[k], reals[k]) * lambdas[k]\n",
    "        # combined loss and calculate gradients\n",
    "        loss += loss_G + loss_cycle + loss_idt\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def discriminator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for the discriminators D_X and D_Y\"\"\"\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    preds_real = cyclegan.discriminate(reals)\n",
    "    # Get fakes from pool\n",
    "    fakes = {k: v.detach() for k, v in fakes.items()}\n",
    "    preds_fake = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        #################### TODO Choice 2 #####################\n",
    "        # OPTION 1\n",
    "        # loss_real = criterionGAN(preds_real[k], True)\n",
    "        # loss_fake = criterionGAN(preds_fake[k], False)\n",
    "        # OPTION 2\n",
    "        # loss_real = criterionGAN(preds_real[k], False)\n",
    "        # loss_fake = criterionGAN(preds_fake[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        loss += (loss_real + loss_fake) * 0.5\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def train_gan(reals):\n",
    "    \"\"\"Optimize the network parameters on a batch of images.\n",
    "\n",
    "    reals: Dict[str, torch.Tensor]\n",
    "        Classname -> Tensor dictionary of images.\n",
    "    \"\"\"\n",
    "    #################### TODO Choice 3 #####################\n",
    "    # OPTION 1\n",
    "    # set_requires_grad(cyclegan.generators, True)\n",
    "    # set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    # set_requires_grad(cyclegan.generators, False)\n",
    "    # set_requires_grad(cyclegan.discriminators, True)\n",
    "    ##########################################################\n",
    "\n",
    "    optimizer_g.zero_grad()\n",
    "    generator_step(cyclegan, reals)\n",
    "    optimizer_g.step()\n",
    "\n",
    "    #################### TODO (still) choice 3 #####################\n",
    "    # OPTION 1\n",
    "    # set_requires_grad(cyclegan.generators, True)\n",
    "    # set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    # set_requires_grad(cyclegan.generators, False)\n",
    "    # set_requires_grad(cyclegan.discriminators, True)\n",
    "    #################################################################\n",
    "\n",
    "    optimizer_d.zero_grad()\n",
    "    discriminator_step(cyclegan, reals)\n",
    "    optimizer_d.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c12f75",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "\n",
    "def set_requires_grad(module, value=True):\n",
    "    \"\"\"Sets `requires_grad` on a `module`'s parameters to `value`\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = value\n",
    "\n",
    "\n",
    "def generator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for generators G_X and G_Y\"\"\"\n",
    "    # Get all generated images\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    # Get discriminator opinion\n",
    "    discrimination = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        # Identity loss\n",
    "        # G_A should be identity if real_B is fed: ||G_A(B) - B||\n",
    "        loss_idt = criterionIdt(identities[k], reals[k]) * lambdas[k] * lambda_idt\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        #################### TODO Choice 1 #####################\n",
    "        # OPTION 1\n",
    "        # loss_G = criterionGAN(discrimination[k], False)\n",
    "        # OPTION2\n",
    "        loss_G = criterionGAN(discrimination[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        # Forward cycle loss || G_B(G_A(A)) - A||\n",
    "        loss_cycle = criterionCycle(reconstructions[k], reals[k]) * lambdas[k]\n",
    "        # combined loss and calculate gradients\n",
    "        loss += loss_G + loss_cycle + loss_idt\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def discriminator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for the discriminators D_X and D_Y\"\"\"\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    preds_real = cyclegan.discriminate(reals)\n",
    "    # Get fakes from pool\n",
    "    fakes = {k: v.detach() for k, v in fakes.items()}\n",
    "    preds_fake = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        #################### TODO Choice 2 #####################\n",
    "        # OPTION 1\n",
    "        loss_real = criterionGAN(preds_real[k], True)\n",
    "        loss_fake = criterionGAN(preds_fake[k], False)\n",
    "        # OPTION 2\n",
    "        # loss_real = criterionGAN(preds_real[k], False)\n",
    "        # loss_fake = criterionGAN(preds_fake[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        loss += (loss_real + loss_fake) * 0.5\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def train_gan(reals):\n",
    "    \"\"\"Optimize the network parameters on a batch of images.\n",
    "\n",
    "    reals: Dict[str, torch.Tensor]\n",
    "        Classname -> Tensor dictionary of images.\n",
    "    \"\"\"\n",
    "    #################### TODO Choice 3 #####################\n",
    "    # OPTION 1\n",
    "    set_requires_grad(cyclegan.generators, True)\n",
    "    set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    # set_requires_grad(cyclegan.generators, False)\n",
    "    # set_requires_grad(cyclegan.discriminators, True)\n",
    "    ##########################################################\n",
    "\n",
    "    optimizer_g.zero_grad()\n",
    "    generator_step(cyclegan, reals)\n",
    "    optimizer_g.step()\n",
    "\n",
    "    #################### TODO (still) choice 3 #####################\n",
    "    # OPTION 1\n",
    "    # set_requires_grad(cyclegan.generators, True)\n",
    "    # set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    set_requires_grad(cyclegan.generators, False)\n",
    "    set_requires_grad(cyclegan.discriminators, True)\n",
    "    #################################################################\n",
    "\n",
    "    optimizer_d.zero_grad()\n",
    "    discriminator_step(cyclegan, reals)\n",
    "    optimizer_d.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc8058",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's add a quick plotting function before we begin training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84792992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_output(sample=None):\n",
    "    # Get the input from the test dataset\n",
    "    if sample is None:\n",
    "        i = np.random.randint(len(gan_test_dataset))\n",
    "        x, y = gan_test_dataset[i]\n",
    "        x = x.to(device)\n",
    "        reals = {classes[y]: x}\n",
    "    else:\n",
    "        reals = sample\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fakes, identities, reconstructions = cyclegan(reals)\n",
    "    inverse_keys = cyclegan.inverse_keys\n",
    "    for k in reals.keys():\n",
    "        inv_k = inverse_keys[k]\n",
    "        for i in range(len(reals[k])):\n",
    "            fig, (ax, ax_fake, ax_id, ax_recon) = plt.subplots(1, 4)\n",
    "            ax.imshow(reals[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_fake.imshow(fakes[inv_k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_id.imshow(identities[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_recon.imshow(reconstructions[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            # Name the axes\n",
    "            ax.set_title(f\"{k.capitalize()}\")\n",
    "            ax_fake.set_title(\"Counterfactual\")\n",
    "            ax_id.set_title(\"Identity\")\n",
    "            ax_recon.set_title(\"Reconstruction\")\n",
    "            for ax in [ax, ax_fake, ax_id, ax_recon]:\n",
    "                ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a561abb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-banner alert-info\"><h4>Exercise 3.2</h4>\n",
    "Training! Let's train the CycleGAN one batch a time, plotting the output every so often to see how it is getting on. This exercise has no TODOs, so just run the next few cells and watch as synapses begin to form.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a balanced sampler that only considers the two classes\n",
    "sampler = balanced_sampler(gan_train_dataset)\n",
    "dataloader = DataLoader(\n",
    "    gan_train_dataset, batch_size=8, drop_last=True, sampler=sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to train for (note: this is not *nearly* enough to get ideal results)\n",
    "iterations = 500\n",
    "# Determines how often to plot outputs to see how the network is doing. I recommend scaling your `print_every` to your `iterations`.\n",
    "# For example, if you're running `iterations=5` you can `print_every=1`, but `iterations=1000` and `print_every=1` will be a lot of prints.\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab45f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    real = get_as_dictionary(x, y)\n",
    "    train_gan(real)\n",
    "    if i % print_every == 0:\n",
    "        plot_gan_output(sample=real)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ca49f",
   "metadata": {},
   "source": [
    "...this time again.\n",
    "![model_train_2.jpeg](attachment:model_train_2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd246c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 3</h1>\n",
    "You know the drill... let us know on Element!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443e2f9",
   "metadata": {},
   "source": [
    "# Part 4: Evaluating the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312b56c",
   "metadata": {},
   "source": [
    "\n",
    "### That was fun!... let's load a pre-trained model\n",
    "\n",
    "Training the CycleGAN takes a lot longer than the few iterations that we did above. Since we don't have that kind of time, we are going to load a pre-trained model (for reference, this pre-trained model was trained for 7 days...).\n",
    "\n",
    "To continue, interrupt the kernel and continue with the next one, which will just use one of the pretrained CycleGAN models for the synapse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, classA, classB):\n",
    "    \"\"\"Load the pre-trained models from the path\"\"\"\n",
    "    directory = Path(path).expanduser() / f\"{classA}_{classB}\"\n",
    "    # Load generators\n",
    "    model.generators[classB].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_G_A.pth\")\n",
    "    )\n",
    "    model.generators[classA].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_G_B.pth\")\n",
    "    )\n",
    "    # Load discriminators\n",
    "    model.discriminators[classA].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_D_A.pth\")\n",
    "    )\n",
    "    model.discriminators[classB].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_D_B.pth\")\n",
    "    )\n",
    "\n",
    "\n",
    "load_pretrained(cyclegan, \"./checkpoints/synapses/cycle_gan/\", *classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3323f4",
   "metadata": {},
   "source": [
    "Let's look at some examples. Can you pick up on the differences between original, the counter-factual, and the reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_gan_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1eb5d",
   "metadata": {},
   "source": [
    "We're going to apply the CycleGAN to our test dataset, and save the results to be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(gan_test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9743a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave\n",
    "\n",
    "\n",
    "def unnormalize(image):\n",
    "    return ((0.5 * image + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_gan(dataloader, directory):\n",
    "    \"\"\"Run CycleGAN on a dataloader and save images to a directory.\"\"\"\n",
    "    directory = Path(directory)\n",
    "    inverse_keys = cyclegan.inverse_keys\n",
    "    cyclegan.eval()\n",
    "    batch_size = dataloader.batch_size\n",
    "    n_sample = 0\n",
    "    for batch, (x, y) in enumerate(tqdm(dataloader)):\n",
    "        reals = get_as_dictionary(x.to(device), y.to(device))\n",
    "        fakes, _, recons = cyclegan(reals)\n",
    "        for k in reals.keys():\n",
    "            inv_k = inverse_keys[k]\n",
    "            (directory / f\"real/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "            (directory / f\"reconstructed/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "            (directory / f\"counterfactual/{inv_k}\").mkdir(parents=True, exist_ok=True)\n",
    "            for i, (im_real, im_fake, im_recon) in enumerate(\n",
    "                zip(reals[k], fakes[inv_k], recons[k])\n",
    "            ):\n",
    "                # Save real synapse images\n",
    "                imsave(\n",
    "                    directory / f\"real/{k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_real.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Save fake synapse images\n",
    "                imsave(\n",
    "                    directory / f\"reconstructed/{k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_recon.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Save counterfactual synapse images\n",
    "                imsave(\n",
    "                    directory / f\"counterfactual/{inv_k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_fake.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Count\n",
    "                n_sample += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea288223",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_gan(dataloader, \"test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61ee80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean-up the gpu's memory a bit to avoid Out-of-Memory errors\n",
    "cyclegan = cyclegan.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fe7fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Exercise 4. Evaluating the GAN\n",
    "\n",
    "The first thing to find out is whether the CycleGAN is successfully converting the images from one neurotransmitter to another.\n",
    "We will do this by running the classifier that we trained earlier on generated data.\n",
    "\n",
    "The data were saved in a directory called `test_images`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(directory):\n",
    "    \"\"\"Create a dataset from a directory of images with the classes in the same order as the VGG's output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: str\n",
    "        The root directory of the images. It should contain sub-directories named after the classes, in which images are stored.\n",
    "    \"\"\"\n",
    "    # Make a dataset with the classes in the correct order\n",
    "    limited_classes = {k: v for k, v in class_to_idx.items() if k in classes}\n",
    "    dataset = ImageFolder(root=directory, transform=transform)\n",
    "    samples = ImageFolder.make_dataset(\n",
    "        directory, class_to_idx=limited_classes, extensions=\".png\"\n",
    "    )\n",
    "    # Sort samples by name\n",
    "    samples = sorted(samples, key=lambda s: s[0].split(\"_\")[-1])\n",
    "    dataset.classes = classes\n",
    "    dataset.class_to_idx = limited_classes\n",
    "    dataset.samples = samples\n",
    "    dataset.targets = [s[1] for s in samples]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def test_gan(dataset):\n",
    "    \"\"\"Evaluate prediction accuracy on the test dataset.\"\"\"\n",
    "    # Run the evaluation\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    return evaluate(dataloader, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27525182",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 4.1 Get the classifier accuracy on CycleGAN outputs</h4>\n",
    "\n",
    "Using the saved images, we're going to figure out how good our CycleGAN is at generating images of a new class!\n",
    "\n",
    "The images (`real`, `reconstructed`, and `counterfactual`) are saved in the `test_images/` directory. Before you start the exercise, have a look at how this directory is organized.\n",
    "\n",
    "TODO\n",
    "- Create a dataset for the three different image types that we saved above\n",
    "    - real\n",
    "    - reconstructed\n",
    "    - counterfactual\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b56ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Dataset of real images\n",
    "ds_real = make_dataset(...)\n",
    "# Dataset of reconstructed images (full cycle)\n",
    "ds_recon = make_dataset(...)\n",
    "# Datset of counterfactuals (half-cycle)\n",
    "ds_counterfactual = make_dataset(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7f1a8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Solution to Exercise 4.2 #\n",
    "############################\n",
    "\n",
    "# Dataset of real images\n",
    "ds_real = make_dataset(\"test_images/real/\")\n",
    "# Dataset of reconstructed images (full cycle)\n",
    "ds_recon = make_dataset(\"test_images/reconstructed/\")\n",
    "# Datset of counterfactuals (half-cycle)\n",
    "ds_counterfactual = make_dataset(\"test_images/counterfactual/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757c202",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-banner alert-info\">\n",
    "We get the following accuracies:\n",
    "\n",
    "1. `accuracy_real`: Accuracy of the classifier on the real images, just for the two classes used in the GAN\n",
    "2. `accuracy_recon`: Accuracy of the classifier on the reconstruction.\n",
    "3. `accuracy_counter`: Accuracy of the classifier on the counterfactual images.\n",
    "\n",
    "<h4>Questions</h4>\n",
    "- In a perfect world, what value would we expect for `accuracy_recon`? What do we compare it to and why is it higher/lower?\n",
    "- How well is it translating from one class to another? The higher `accuracy_counter` the better, but...\n",
    "- Anything better than `1 - accuracy_real` is a success! Why?\n",
    "\n",
    "Let us know your insights on Element.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b42b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_real = test_gan(ds_real)\n",
    "accuracy_recon = test_gan(ds_recon)\n",
    "accuracy_counter = test_gan(ds_counterfactual)\n",
    "print(\n",
    "    f\"Accuracy real: {accuracy_real}\\nAccuracy reconstruction: {accuracy_recon}\\nAccuracy counterfactuals: {accuracy_counter}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad6565",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>TODO</h4>\n",
    "- Modify the `evaluate_cm` method so that it also returns the probability of the target class (hint: you might need to `zip`, and you can use `item()` to get a value from a single-element tensor).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c911f60",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 4</h1>\n",
    " We have seen that our CycleGAN network has successfully translated some of the synapses from one class to the other, but there are clearly some things to look out for!\n",
    "\n",
    "Take the time to think about these questions before moving on:\n",
    "- What would you expect the confusion matrix for the counterfactuals to look like? Why?\n",
    "- Do the two directions of the CycleGAN work equally as well?\n",
    "- Can you think of anything that might have made it more difficult, or easier, to translate in a one direction vs the other?\n",
    "\n",
    "This is the end of Section 3. Let us know on Element if you have reached this point!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaa329",
   "metadata": {},
   "source": [
    "## Part 5: Highlighting Class-Relevant Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfecce1",
   "metadata": {},
   "source": [
    "At this point we have:\n",
    "- A classifier that can differentiate between neurotransmitters from EM images of synapses\n",
    "- A vague idea of which parts of the images it thinks are important for this classification\n",
    "- A CycleGAN that is sometimes able to trick the classifier with barely perceptible changes\n",
    "\n",
    "What we don't know, is *how* the CycleGAN is modifying the images to change their class.\n",
    "\n",
    "To start to answer this question, we will use a [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412) method to highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78c06a",
   "metadata": {},
   "source": [
    "### Exercise 5 Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33a61a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 5.1 Get sucessfully converted samples</h4>\n",
    "The CycleGAN is able to convert some, but not all images into their target types.\n",
    "In order to observe and highlight useful differences, we want to observe our attribution method at work only on those examples of synapses:\n",
    "<ol>\n",
    "    <li> That were correctly classified originally</li>\n",
    "    <li>Whose counterfactuals were also correctly classified</li>\n",
    "</ol>\n",
    "\n",
    "TODO\n",
    "- Create a score based on the probabilities we got out of `evaluate_cm_gan` that covers these two conditions (hint: `+`)\n",
    "- Sort the items by how good they are (hint: `argsort`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fbff2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "####### Exercise 5.1 TODO #######\n",
    "score = ...  # How good they were before + how good they became\n",
    "best_to_worst = ...  # Sort them by how good they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37158b0c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Solution to Exercise 5.1 #\n",
    "########################\n",
    "# How good they were before + how good they became\n",
    "score = torch.tensor(gan_probs) + torch.tensor(probs)\n",
    "best_to_worst = torch.argsort(torch.tensor(score), descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869c66c",
   "metadata": {},
   "source": [
    "To check that we have got it right, let us get the accuracy on the best 100 vs the worst 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_100 = Subset(ds_counterfactual, best_to_worst[:100])\n",
    "best_100_real = Subset(ds_real, best_to_worst[:100])\n",
    "worst_100 = Subset(ds_counterfactual, best_to_worst[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(best_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b86dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(worst_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c94259",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Exercise 5.2 Creating hybrids from attributions</h4>\n",
    "\n",
    "Now that we have a set of successfully translated counterfactuals, we can use them as a baseline for our attribution.\n",
    "If you remember from earlier, `IntegratedGradients` does a interpolation between the model gradients at the baseline and the model gradients at the sample. Here, we're also going to be doing an interpolation between the baseline image and the sample image, creating a hybrid!\n",
    "\n",
    "To do this, we will take the sample image and mask out all of the pixels in the attribution. We will then replace these masked out pixels by the equivalent values in the counterfactual. So we'll have a hybrid image that is like the original everywhere except in the areas that matter for classification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_real = DataLoader(best_100_real, batch_size=10)\n",
    "dataloader_counter = DataLoader(best_100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    # Create an integrated gradients object.\n",
    "    # integrated_gradients = IntegratedGradients(model)\n",
    "    # Generated attributions on integrated gradients\n",
    "    attributions = np.vstack(\n",
    "        [\n",
    "            integrated_gradients.attribute(\n",
    "                real.to(device),\n",
    "                target=target.to(device),\n",
    "                baselines=counterfactual.to(device),\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            for (real, target), (counterfactual, _) in zip(\n",
    "                dataloader_real, dataloader_counter\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    # Normalize the attributions\n",
    "    attributions = [a / np.max(np.abs(a)) for a in attributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for creating an interactive visualization of our attributions\n",
    "model.cpu()\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"viridis\")\n",
    "colors = cmap([0, 255])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_classifications(image, counter, hybrid):\n",
    "    model.eval()\n",
    "    class_idx = [full_dataset.classes.index(c) for c in classes]\n",
    "    tensor = torch.from_numpy(np.stack([image, counter, hybrid])).float()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)[:, class_idx]\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred, counter_pred, hybrid_pred = probs\n",
    "    return pred.numpy(), counter_pred.numpy(), hybrid_pred.numpy()\n",
    "\n",
    "\n",
    "def interactive_counterfactuals(idx, threshold=0.1):\n",
    "    image = best_100_real[idx][0].numpy()\n",
    "    counter = best_100[idx][0].numpy()\n",
    "    mask = get_mask(attributions[idx], threshold)\n",
    "    hybrid = (1 - mask) * image + mask * counter\n",
    "    nan_mask = copy.deepcopy(mask)\n",
    "    nan_mask[nan_mask != 0] = 1\n",
    "    nan_mask[nan_mask == 0] = np.nan\n",
    "    # PLOT\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"\"\"\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   ....ggg.fff.ppp\n",
    "                                   \"\"\",\n",
    "        figsize=(20, 5),\n",
    "    )\n",
    "    # Original\n",
    "    axes[\"m\"].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axes[\"m\"].imshow(nan_mask.squeeze(), cmap=\"YlOrRd\", vmin=0, vmax=1, alpha=0.5)\n",
    "    axes[\"m\"].set_title(\"Mask\", fontsize=24)\n",
    "    # Original\n",
    "    axes[\"o\"].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axes[\"o\"].set_title(\"Original\", fontsize=24)\n",
    "    # Counterfactual\n",
    "    axes[\"c\"].imshow(counter.squeeze(), cmap=\"gray\")\n",
    "    axes[\"c\"].set_title(\"Counterfactual\", fontsize=24)\n",
    "    # Hybrid\n",
    "    axes[\"h\"].imshow(hybrid.squeeze(), cmap=\"gray\")\n",
    "    axes[\"h\"].set_title(\"Hybrid\", fontsize=24)\n",
    "    # Mask\n",
    "    pred, counter_pred, hybrid_pred = get_classifications(image, counter, hybrid)\n",
    "    axes[\"g\"].barh(classes, pred, color=colors)\n",
    "    axes[\"f\"].barh(classes, counter_pred, color=colors)\n",
    "    axes[\"p\"].barh(classes, hybrid_pred, color=colors)\n",
    "    for ix in [\"m\", \"o\", \"c\", \"h\"]:\n",
    "        axes[ix].axis(\"off\")\n",
    "\n",
    "    for ix in [\"g\", \"f\", \"p\"]:\n",
    "        for tick in axes[ix].get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "        axes[ix].set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffe4b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>TODO</h4>\n",
    "Below is a small widget to interact with the above analysis. As you change the `threshold`, see how the prediction of the hybrid changes.\n",
    "At what point does it swap over?\n",
    "If you want to see different samples, slide through the `idx`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543827",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(interactive_counterfactuals, idx=(0, 99), threshold=(0.0, 1.0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf86b44",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>Questions</h4>\n",
    "- Can you find features that define either of the two classes?\n",
    "-  How consistent are they across the samples?\n",
    "-  Is there a range of thresholds where most of the hybrids swap over to the target class? (If you want to see that area, try to change the range of thresholds in the slider by setting `threshold=(minimum_value, maximum_value, step_size)`\n",
    "\n",
    "Feel free to discuss your answers on Element!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b180b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1>The End.</h1>\n",
    "    Go forth and train some GANs!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24b18e",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "Here are some ideas for how to continue with this notebook:\n",
    "\n",
    "1. Improve the classifier. This code uses a VGG network for the classification. On the synapse dataset, we will get a validation accuracy of around 80%. Try to see if you can improve the classifier accuracy.\n",
    "    * (easy) Data augmentation: The training code for the classifier is quite simple in this example. Enlarge the amount of available training data by adding augmentations (transpose and mirror the images, add noise, change the intensity, etc.).\n",
    "    * (easy) Network architecture: The VGG network has a few parameters that one can tune. Try a few to see what difference it makes.\n",
    "    * (easy) Inspect the classifier predictions: Take random samples from the test dataset and classify them. Show the images together with their predicted and actual labels.\n",
    "    * (medium) Other networks:  Try different architectures (e.g., a [ResNet](https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/#resnet-from-scratch)) and see if the accuracy can be improved.\n",
    "\n",
    "2. Explore the CycleGAN.\n",
    "    * (easy) The example code below shows how to translate between GABA and acetylcholine. Try different combinations. Can you start to see differences between some pairs of classes? Which are the ones where the differences are the most or the least obvious? Can you see any differences that aren't well described by the mask? How would you describe these?\n",
    "\n",
    "3. Try on your own data!\n",
    "    * Have a look at how the synapse images are organized in `data/raw/synapses`. Copy the directory structure and use your own images. Depending on your data, you might have to adjust the image size (128x128 for the synapses) and number of channels in the VGG network and CycleGAN code."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
