{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f0f7f9",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 9: Knowledge Extraction from a Convolutional Neural Network\n",
    "\n",
    "In the following exercise we will train a convolutional neural network to classify electron microscopy images of Drosophila synapses, based on which neurotransmitter they contain. We will then train a CycleGAN and use a method called Discriminative Attribution from Counterfactuals (DAC) to understand how the network performs its classification, effectively going back from prediction to image data.\n",
    "\n",
    "![synister.png](assets/synister.png)\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook was written by Jan Funke and modified by Tri Nguyen and Diane Adjavon, using code from Nils Eckstein and a modified version of the [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25e710",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>09_knowledge_extraction</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b96c13",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Start here (AKA checkpoint 0)</h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c339e3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 1: Image Classification\n",
    "\n",
    "## Training an image classifier\n",
    "In this section, we will implement and train a VGG classifier to classify images of synapses into one of six classes, corresponding to the neurotransmitter type that is released at the synapse: GABA, acethylcholine, glutamate, octopamine, serotonin, and dopamine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524106",
   "metadata": {},
   "source": [
    "\n",
    "The data we use for this exercise is located in `data/raw/synapses`, where we have one subdirectory for each neurotransmitter type. Look at a few examples to familiarize yourself with the dataset. You will notice that the dataset is not balanced, i.e., we have much more examples of one class versus another one.\n",
    "\n",
    "This class imbalance is problematic for training a classifier. Imagine that 99% of our images are of one class, then the classifier would do really well predicting this class all the time, without having learnt anything of substance. It is therefore important to balance the dataset, i.e., present the same number of images per class to the classifier during training.\n",
    "\n",
    "First, we split the available images into a train, validation, and test dataset with proportions of 0.7, 0.15, and 0.15, respectively. Each image should be returned as a 2D `numpy` array with float values between 0 and 1. The label for each image should be the name of the directory for this class (e.g., `0_gaba`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1c9b7",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create a dataset for all images of all classes\n",
    "full_dataset = ImageFolder(root=\"data/raw/synapses\", transform=transform)\n",
    "\n",
    "# Rename the classes\n",
    "full_dataset.classes = [x.split(\"_\")[-1] for x in full_dataset.classes]\n",
    "class_to_idx = {x.split(\"_\")[-1]: v for x, v in full_dataset.class_to_idx.items()}\n",
    "full_dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# randomly split the dataset into train, validation, and test\n",
    "num_images = len(full_dataset)\n",
    "# ~70% for training\n",
    "num_training = int(0.7 * num_images)\n",
    "# ~15% for validation\n",
    "num_validation = int(0.15 * num_images)\n",
    "# ~15% for testing\n",
    "num_test = num_images - (num_training + num_validation)\n",
    "# split the data randomly (but with a fixed random seed)\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [num_training, num_validation, num_test],\n",
    "    generator=torch.Generator().manual_seed(23061912),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f148f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating a Balanced Dataloader\n",
    "\n",
    "Below define a `sampler` that samples images of classes with skewed probabilities to account for the different number of items in each class.\n",
    "\n",
    "The sampler\n",
    "- Counts the number of samples in each class\n",
    "- Gets the weight-per-label as an inverse of the frequency\n",
    "- Get the weight-per-sample\n",
    "- Create a `WeightedRandomSampler` based on these weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute class weights in training dataset for balanced sampling\n",
    "def balanced_sampler(dataset):\n",
    "    # Get a list of targets from the dataset\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        # A Subset is a specific type of dataset, which does not directly have access to the targets.\n",
    "        targets = torch.tensor(dataset.dataset.targets)[dataset.indices]\n",
    "    else:\n",
    "        targets = dataset.targets\n",
    "\n",
    "    counts = torch.bincount(targets)  # Count the number of samples for each class\n",
    "    label_weights = (\n",
    "        1.0 / counts\n",
    "    )  # Get the weight-per-label as an inverse of the frequency\n",
    "    weights = label_weights[targets]  # Get the weight-per-sample\n",
    "\n",
    "    # Optional: Print the Counts and Weights to make sure lower frequency classes have higher weights\n",
    "    print(\"Number of images per class:\")\n",
    "    for c, n, w in zip(full_dataset.classes, counts, label_weights):\n",
    "        print(f\"\\t{c}:\\tn={n}\\tweight={w}\")\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights, len(weights)\n",
    "    )  # Create a sampler based on these weights\n",
    "    return sampler\n",
    "\n",
    "\n",
    "sampler = balanced_sampler(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0525e",
   "metadata": {},
   "source": [
    "We make a `torch` `DataLoader` that takes our `sampler` to create batches of eight images and their corresponding labels.\n",
    "Each image should be randomly and equally selected from the six available classes (i.e., for each image sample pick a random class, then pick a random image from this class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b4bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this data loader will serve 8 images in a \"mini-batch\" at a time\n",
    "dataloader = DataLoader(train_dataset, batch_size=8, drop_last=True, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892ab7f",
   "metadata": {},
   "source": [
    "The cell below visualizes a single, randomly chosen batch from the training data loader. Feel free to execute this cell multiple times to get a feeling for the dataset and that your sampler gives batches of evenly distributed synapse types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab255a",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_batch(x, y):\n",
    "    fig, axs = plt.subplots(1, x.shape[0], figsize=(14, 14), sharey=True)\n",
    "    for i in range(x.shape[0]):\n",
    "        axs[i].imshow(np.squeeze(x[i]), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axs[i].set_title(train_dataset.dataset.classes[y[i].item()])\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# show a random batch from the data loader\n",
    "# (run this cell repeatedly to see different batches)\n",
    "for x, y in dataloader:\n",
    "    show_batch(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025648fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating a VGG Network, Loss\n",
    "\n",
    "We will use a VGG network to classify the synapse images. The input to the network will be a 2D image as provided by your dataloader. The output will be a vector of six floats, corresponding to the probability of the input to belong to the six classes.\n",
    "\n",
    "We have implemented a VGG network below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2b968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vgg2D(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        fmaps=12,\n",
    "        downsample_factors=[(2, 2), (2, 2), (2, 2), (2, 2)],\n",
    "        output_classes=6,\n",
    "    ):\n",
    "        super(Vgg2D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        current_fmaps, h, w = tuple(input_size)\n",
    "        current_size = (h, w)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "            features += [\n",
    "                torch.nn.Conv2d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(fmaps, fmaps, kernel_size=3, padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool2d(downsample_factors[i]),\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c / d) for c, d in zip(current_size, downsample_factors[i])\n",
    "            )\n",
    "            check = (\n",
    "                s * d == c for s, d, c in zip(size, downsample_factors[i], current_size)\n",
    "            )\n",
    "            assert all(check), \"Can not downsample %s by chosen downsample factor\" % (\n",
    "                current_size,\n",
    "            )\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] * current_size[1] * current_fmaps, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, output_classes),\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "    def forward(self, raw):\n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "\n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544bd0d",
   "metadata": {},
   "source": [
    "We'll start by creating the VGG with the default parameters and push it to a GPU if there is one available. Then we'll define the training loss and optimizer.\n",
    "The training and evaluation loops have been defined for you, so after that just train your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fca99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the size of our images\n",
    "for x, y in train_dataset:\n",
    "    input_size = x.shape\n",
    "    break\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg2D(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929dd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a GPU, if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2d8ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1.1: Train the VGG Network</h3>\n",
    "\n",
    "- Choose a loss\n",
    "- Create an Adam optimizer and set its learning rate\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29af1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb96afe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The next cell defines some convenience functions for training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f21c05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "\n",
    "    # set the model into train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    for x, y in tqdm(dataloader, \"train\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += l\n",
    "        num_batches += 1\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c473df0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1.2: Create a prediction function</h3>\n",
    "\n",
    "To understand the performance of the classifier, we need to run predictions on the validation dataset so that we can get accuracy during training, and eventually a confusiom natrix. In practice, this will allow us to stop before we overfit, although in this exercise we will probably not be training that long. Then, later, we can use the same prediction function on test data.\n",
    "\n",
    "\n",
    "TODO\n",
    "Modify `predict` so that it returns a paired list of predicted class vs ground truth to produce a confusion matrix. You'll need to do the following steps.\n",
    "- Get the model output for the batch of data `(x, y)`\n",
    "- Turn the model output into a probability\n",
    "- Get the class predictions from the probabilities\n",
    "- Add the class predictions to a list of all predictions\n",
    "- Add the ground truths to a list of all ground truths\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae63f62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: return a paired list of predicted class vs ground-truth to produce a confusion matrix\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def predict(dataset, name, batch_size=32):\n",
    "    # These data laoders serve images in a \"mini-batch\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, drop_last=False)\n",
    "    #\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Get the model output\n",
    "        # Turn the model output into a probability\n",
    "        # Get the class predictions from the probabilities\n",
    "\n",
    "        predictions.extend(...)  # TODO add predictions to the list\n",
    "        ground_truths.extend(...)  # TODO add ground truths to the list\n",
    "    return np.array(predictions), np.array(ground_truths)\n",
    "\n",
    "\n",
    "prediction, ground_truth = predict(test_dataset, \"Test\")\n",
    "print(\"Current test accuracy of the network\", accuracy_score(ground_truth, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee4910",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We are ready to train. After each epoch (roughly going through each training image once), we report the training loss and the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc31bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    epoch_loss = train(dataloader)\n",
    "    print(f\"Epoch {epoch}, training loss={epoch_loss}\")\n",
    "\n",
    "    predictions, gt = predict(validation_dataset, \"Validation\")\n",
    "    accuracy = accuracy_score(gt, predictions)\n",
    "    print(f\"Epoch {epoch}, validation accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91973f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's watch your model train!\n",
    "\n",
    "<img src=\"assets/model_train.jpg\" alt=\"drawing\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324a440",
   "metadata": {},
   "source": [
    "And now, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0770ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions, ground_truths = predict(test_dataset, \"Test\")\n",
    "accuracy = accuracy_score(ground_truths, predictions)\n",
    "print(f\"Final test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57241755",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If you're unhappy with the accuracy above (which you should be...) we pre-trained a model for you for many more epochs. You can load it with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cad3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Run this cell if you want a shortcut\n",
    "yes_I_want_the_pretrained_model = True\n",
    "\n",
    "if yes_I_want_the_pretrained_model:\n",
    "    checkpoint = torch.load(\n",
    "        \"checkpoints/synapses/classifier/vgg_checkpoint\", map_location=device\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "# And check the (hopefully much better) accuracy\n",
    "predictions, ground_truths = predict(test_dataset, \"Test\")\n",
    "accuracy = accuracy_score(ground_truths, predictions)\n",
    "print(f\"Final_final_v2_last_one test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d26644",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Constructing a confusion matrix\n",
    "\n",
    "We now have a classifier that can discriminate between images of different types. If you used the images we provided, the classifier is not perfect (you should get an accuracy of around 80%), but pretty good considering that there are six different types of images.\n",
    "\n",
    "To understand the performance of the classifier beyond a single accuracy number, we should build a confusion matrix that can more elucidate which classes are more/less misclassified and which classes are those wrong predictions confused with.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae027f",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc315793",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "# orginally from Runqi Yang;\n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, names, labels=None, title=None, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "      confusion_matrix: np.array\n",
    "      labels: list\n",
    "          List of integer values to determine which classes to consider.\n",
    "      names:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if labels is not None:\n",
    "        assert len(names) == len(labels)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = \"%.1f%%\\n%d/%d\" % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = \"\"\n",
    "            else:\n",
    "                annot[i, j] = \"%.1f%%\\n%d\" % (p, c)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = sns.heatmap(\n",
    "        cm_perc, annot=annot, fmt=\"\", vmax=100, xticklabels=names, yticklabels=names\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "names = [\"gaba\", \"acetylcholine\", \"glutamate\", \"serotonine\", \"octopamine\", \"dopamine\"]\n",
    "cm_analysis(predictions, ground_truths, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cf7bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "<h4>Questions</h4>\n",
    "\n",
    "- What observations can we make from the confusion matrix?\n",
    "- Does the classifier do better on some synapse classes than other?\n",
    "- If you have time later, which ideas would you try to train a better predictor?\n",
    "\n",
    "Let us know your thoughts on the course chat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ccb36",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 1</h2>\n",
    "\n",
    "We now have:\n",
    "- A classifier that is pretty good at predicting neurotransmitters from EM images.\n",
    "\n",
    "This is surprising, since we could not (yet) have made these predictions manually! If you're skeptical, feel free to explore the data a bit more and see for yourself if you can tell the difference betwee, say, GABAergic and glutamatergic synapses.\n",
    "\n",
    "So this is an interesting situation: The VGG network knows something we don't quite know. In the next section, we will see how we can find and then visualize the relevant differences between images of different types.\n",
    "\n",
    "This concludes the first section. Let us know on the exercise chat if you have arrived here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f14b2",
   "metadata": {},
   "source": [
    "# Part 2: Masking the relevant part of the image\n",
    "\n",
    "In this section we will make a first attempt at highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41464574",
   "metadata": {},
   "source": [
    "## Attributions through integrated gradients\n",
    "\n",
    "Attribution is the process of finding out, based on the output of a neural network, which pixels in the input are (most) responsible. Another way of thinking about it is: which pixels would need to change in order for the network's output to change.\n",
    "\n",
    "Here we will look at an example of an attribution method called [Integrated Gradients](https://captum.ai/docs/extension/integrated_gradients). If you have a bit of time, have a look at this [super fun exploration of attribution methods](https://distill.pub/2020/attribution-baselines/), especially the explanations on Integrated Gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08ae72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf1572",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.1 Get an attribution</h3>\n",
    "\n",
    "In this next part, we will get attributions on single batch. We use a library called [captum](https://captum.ai), and focus on the `IntegratedGradients` method.\n",
    "Create an `IntegratedGradients` object and run attribution on `x,y` obtained above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897dd327",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "############### Task 2.1 TODO ############\n",
    "# Create an integrated gradients object.\n",
    "integrated_gradients = ...\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa10dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributions = (\n",
    "    attributions.cpu().numpy()\n",
    ")  # Move the attributions from the GPU to the CPU, and turn then into numpy arrays for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bf893",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here is an example for an image, and its corresponding attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4faa92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "\n",
    "def unnormalize(image):\n",
    "    return 0.5 * image + 0.5\n",
    "\n",
    "\n",
    "def visualize_attribution(attribution, original_image):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(unnormalize(original_image), (1, 2, 0))\n",
    "\n",
    "    viz.visualize_image_attr_multiple(\n",
    "        attribution,\n",
    "        original_image,\n",
    "        methods=[\"blended_heat_map\", \"heat_map\"],\n",
    "        signs=[\"absolute_value\", \"absolute_value\"],\n",
    "        show_colorbar=True,\n",
    "        titles=[\"Original and Attribution\", \"Attribution\"],\n",
    "        use_pyplot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d050712",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd418b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Smoothing the attribution into a mask\n",
    "\n",
    "The attributions that we see are grainy and difficult to interpret because they are a pixel-wise attribution. We apply some smoothing and thresholding on the attributions so that they represent region masks rather than pixel masks. The following code is runnable with no modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55715f0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import copy\n",
    "\n",
    "\n",
    "def smooth_attribution(attrs, struc=10, sigma=11):\n",
    "    # Morphological closing and Gaussian Blur\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (struc, struc))\n",
    "    mask = cv2.morphologyEx(attrs[0], cv2.MORPH_CLOSE, kernel)\n",
    "    mask_cp = copy.deepcopy(mask)\n",
    "    mask_weight = cv2.GaussianBlur(mask_cp.astype(float), (sigma, sigma), 0)\n",
    "    return mask_weight[np.newaxis]\n",
    "\n",
    "\n",
    "def get_mask(attrs, threshold=0.5):\n",
    "    smoothed = smooth_attribution(attrs)\n",
    "    return smoothed > (threshold * smoothed.max())\n",
    "\n",
    "\n",
    "def interactive_attribution(idx=0):\n",
    "    image = x[idx].cpu().numpy()\n",
    "    attrs = attributions[idx]\n",
    "    mask = smooth_attribution(attrs)\n",
    "    visualize_attribution(mask, image)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33598839",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.2 Visualizing the results</h3>\n",
    "\n",
    "The code above creates a small widget to interact with the results of this analysis. Look through the samples for a while before answering the questions below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490db899",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "interact(\n",
    "    interactive_attribution,\n",
    "    idx=(0, dataloader.batch_size - 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dce2c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "HELP! I Can't see any interactive setup!!\n",
    "\n",
    "I got you... just uncomment the next cell and run it to see all of the samples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda303d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HELP! I Can't see any interative setup!!!\n",
    "# for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "#    visualize_attribution(smooth_attribution(attr), im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc4c08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "<h4>Questions</h4>\n",
    "\n",
    "- Are there some recognisable objects or parts of the synapse that show up in several examples?\n",
    "- Are there some objects that seem secondary because they are less strongly highlighted?\n",
    "\n",
    "Tell us what you see on the chat!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34722b",
   "metadata": {},
   "source": [
    "\n",
    "### Changing the basline\n",
    "\n",
    "Many existing attribution algorithms are comparative: they show which pixels of the input are responsible for a network output *compared to a baseline*.\n",
    "The baseline is often set to an all 0 tensor, but the choice of the baseline affects the output.\n",
    "(For an interactive illustration of how the baseline affects the output, see [this Distill paper](https://distill.pub/2020/attribution-baselines/))\n",
    "\n",
    "You can change the baseline used by the `integrated_gradients` object.\n",
    "\n",
    "Use the command:\n",
    "```\n",
    "?integrated_gradients.attribute\n",
    "```\n",
    "To get more details about how to include the baseline.\n",
    "\n",
    "Try using the code above to change the baseline and see how this affects the output.\n",
    "\n",
    "1. Random noise as a baseline\n",
    "2. A blurred/noisy version of the original image as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53feb16f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.3: Use random noise as a baseline</h4>\n",
    "\n",
    "Hint: `torch.rand_like`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c65e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "random_baselines = ...  # TODO Change\n",
    "# Generate the attributions\n",
    "attributions_random = integrated_gradients.attribute(...)  # TODO Change\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_random.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97700bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.4: Use a blurred image a baseline</h4>\n",
    "\n",
    "Hint: `torchvision.transforms.functional` has a useful function for this ;)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5b23e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Import required function\n",
    "\n",
    "# Baseline\n",
    "blurred_baselines = ...  # TODO Create blurred version of the images\n",
    "# Generate the attributions\n",
    "attributions_blurred = integrated_gradients.attribute(...)  # TODO Fill\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_blurred.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdde305",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"altert alert-block alert-warning\"><h4> Questions </h4>\n",
    "\n",
    "- Are any of the features consistent across baselines? Why do you think that is?\n",
    "- What baseline do you like best so far? Why?\n",
    "- If you were to design an ideal baseline, what would you choose?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15cf83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h2>BONUS Task: Using different attributions.</h2>\n",
    "\n",
    "\n",
    "\n",
    "[`captum`](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) has access to various different attribution algorithms.\n",
    "\n",
    "Replace `IntegratedGradients` with different attribution methods. Are they consistent with each other?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8d816",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 2</h2>\n",
    "Let us know on the exercise chat when you've reached this point!\n",
    "\n",
    "At this point we have:\n",
    "\n",
    "- Trained a classifier that can predict neurotransmitters from EM-slices of synapses.</li>\n",
    "- Found a way to mask the parts of the image that seem to be relevant for the classification, using integrated gradients.</li>\n",
    "- Discovered the effect of changing the baseline on the output of integrated gradients.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ef8d6",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 3: Train a GAN to Translate Images\n",
    "\n",
    "To gain insight into how the trained network classify images, we will use [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412), a feature attribution with counterfactual explanations methodology. This method employs a CycleGAN to translate images from one class to another to make counterfactual explanations.\n",
    "\n",
    "**What is a counterfactual?**\n",
    "\n",
    "You've learned about adversarial examples in the lecture on failure modes. These are the imperceptible or noisy changes to an image that drastically changes a classifier's opinion.\n",
    "Counterfactual explanations are the useful cousins of adversarial examples. They are *perceptible* and *informative* changes to an image that changes a classifier's opinion.\n",
    "\n",
    "In the image below you can see the difference between the two. In the first column are MNIST images along with their classifictaions, and in the second column are counterfactual explanations to *change* that class. You can see that in both cases a human being would (hopefully) agree with the new classification. By comparing the two columns, we can therefore begin to define what makes each digit special.\n",
    "\n",
    "In contrast, the third and fourth columns show an MNIST image and a corresponding adversarial example. Here the network returns a prediction that most human beings (who aren't being facetious) would strongly disagree with.\n",
    "\n",
    "<img src=\"assets/ce_vs_ae.png\" width=50% />\n",
    "\n",
    "**Counterfactual synapses**\n",
    "\n",
    "In this example, we will train a CycleGAN network that translates GABAergic synapses to acetylcholine synapses (you can also train other pairs too by changing the classes below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089850c",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def class_dir(name):\n",
    "    return f\"{class_to_idx[name]}_{name}\"\n",
    "\n",
    "\n",
    "classes = [\"gaba\", \"acetylcholine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b89586",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Training a GAN\n",
    "\n",
    "Yes, really!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1b90b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div><h3>Creating a specialized dataset</h3>\n",
    "\n",
    "The CycleGAN works on only 2 classes at a time, but our full dataset has 6. Below, we will use the `Subset` dataset from `torch.utils.data` to get the data from these two classes.\n",
    "\n",
    "A `Subset` is created as follows:\n",
    "```\n",
    "subset = Subset(dataset, indices)\n",
    "```\n",
    "\n",
    "And the chosen indices can be obtained again using `subset.indices`.\n",
    "\n",
    "Run the cell below to generate the datasets:\n",
    "- `gan_train_dataset`\n",
    "- `gan_test_dataset`\n",
    "- `gan_val_dataset`\n",
    "\n",
    "We will use them below to train the CycleGAN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8981d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions to get a subset of classes\n",
    "def get_indices(dataset, classes):\n",
    "    \"\"\"Get the indices of elements of classA and classB in the dataset.\"\"\"\n",
    "    indices = []\n",
    "    for cl in classes:\n",
    "        indices.append(torch.tensor(dataset.targets) == class_to_idx[cl])\n",
    "    logical_or = sum(indices) > 0\n",
    "    return torch.where(logical_or)[0]\n",
    "\n",
    "\n",
    "def set_intersection(a_indices, b_indices):\n",
    "    \"\"\"Get intersection of two sets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_indices: torch.Tensor\n",
    "    b_indices: torch.Tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    intersection: torch.Tensor\n",
    "        The elements contained in both a_indices and b_indices.\n",
    "    \"\"\"\n",
    "    a_cat_b, counts = torch.cat([a_indices, b_indices]).unique(return_counts=True)\n",
    "    intersection = a_cat_b[torch.where(counts.gt(1))]\n",
    "    return intersection\n",
    "\n",
    "\n",
    "# Getting training, testing, and validation indices\n",
    "gan_idx = get_indices(full_dataset, classes)\n",
    "\n",
    "gan_train_idx = set_intersection(torch.tensor(train_dataset.indices), gan_idx)\n",
    "gan_test_idx = set_intersection(torch.tensor(test_dataset.indices), gan_idx)\n",
    "gan_val_idx = set_intersection(torch.tensor(validation_dataset.indices), gan_idx)\n",
    "\n",
    "# Checking that the subsets are complete\n",
    "assert len(gan_train_idx) + len(gan_test_idx) + len(gan_val_idx) == len(gan_idx)\n",
    "\n",
    "# Generate three datasets based on the above indices.\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "gan_train_dataset = Subset(full_dataset, gan_train_idx)\n",
    "gan_test_dataset = Subset(full_dataset, gan_test_idx)\n",
    "gan_val_dataset = Subset(full_dataset, gan_val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b5de4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The model\n",
    "\n",
    "![cycle.png](assets/cyclegan.png)\n",
    "\n",
    "In the following, we create a [CycleGAN model](https://arxiv.org/pdf/1703.10593.pdf). It is a Generative Adversarial model that is trained to turn one class of images X (for us, GABA) into a different class of images Y (for us, Acetylcholine).\n",
    "\n",
    "It has two generators:\n",
    "   - Generator G takes a GABA image and tries to turn it into an image of an Acetylcholine synapse. When given an image that is already showing an Acetylcholine synapse, G should just re-create the same image: these are the `identities`.\n",
    "   - Generator F takes a Acetylcholine image and tries to turn it into an image of an GABA synapse. When given an image that is already showing a GABA synapse, F should just re-create the same image: these are the `identities`.\n",
    "\n",
    "\n",
    "When in training mode, the CycleGAN will also create a `reconstruction`. These are images that are passed through both generators.\n",
    "For example, a GABA image will first be transformed by G to Acetylcholine, then F will turn it back into GABA.\n",
    "This is achieved by training the network with a cycle-consistency loss. In our example, this is an L2 loss between the `real` GABA image and the `reconstruction` GABA image.\n",
    "\n",
    "But how do we force the generators to change the class of the input image? We use a discriminator for each.\n",
    "   - DX tries to recognize fake GABA images: F will need to create images realistic and GABAergic enough to trick it.\n",
    "   - DY tries to recognize fake Acetylcholine images: G will need to create images realistic and cholinergic enough to trick it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308b66b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import functools\n",
    "from cycle_gan.models.networks import ResnetGenerator, NLayerDiscriminator, GANLoss\n",
    "\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    \"\"\"Cycle GAN\n",
    "\n",
    "    Has:\n",
    "    - Two class names\n",
    "    - Two Generators\n",
    "    - Two Discriminators\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, class1, class2, input_nc=1, output_nc=1, ngf=64, ndf=64, use_dropout=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        class1: str\n",
    "            Label of the first class\n",
    "        class2: str\n",
    "            Label of the second class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        norm_layer = functools.partial(\n",
    "            nn.InstanceNorm2d, affine=False, track_running_stats=False\n",
    "        )\n",
    "        self.classes = [class1, class2]\n",
    "        self.inverse_keys = {\n",
    "            class1: class2,\n",
    "            class2: class1,\n",
    "        }  # i.e. what is the other key?\n",
    "        self.generators = nn.ModuleDict(\n",
    "            {\n",
    "                classname: ResnetGenerator(\n",
    "                    input_nc,\n",
    "                    output_nc,\n",
    "                    ngf,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_dropout=use_dropout,\n",
    "                    n_blocks=9,\n",
    "                )\n",
    "                for classname in self.classes\n",
    "            }\n",
    "        )\n",
    "        self.discriminators = nn.ModuleDict(\n",
    "            {\n",
    "                classname: NLayerDiscriminator(\n",
    "                    input_nc, ndf, n_layers=3, norm_layer=norm_layer\n",
    "                )\n",
    "                for classname in self.classes\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        \"\"\"Creates fakes from the reals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict\n",
    "            classname -> images\n",
    "        train: boolean\n",
    "            If false, only the counterfactuals are generated and returned.\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fakes: dict\n",
    "            classname -> images of counterfactual images\n",
    "        identities: dict\n",
    "            classname -> images of images passed through their corresponding generator, if train is True\n",
    "            For example, images of class1 are passed through the generator that creates class1.\n",
    "            These should be identical to the input.\n",
    "            Not returned if `train` is `False`\n",
    "        reconstructions\n",
    "            classname -> images of reconstructed images (full cycle), if train is True.\n",
    "            Not returned if `train` is `False`\n",
    "        \"\"\"\n",
    "        fakes = {}\n",
    "        identities = {}\n",
    "        reconstructions = {}\n",
    "        for k, batch in x.items():\n",
    "            inv_k = self.inverse_keys[k]\n",
    "            # Counterfactual: class changes\n",
    "            fakes[inv_k] = self.generators[inv_k](batch)\n",
    "            if train:\n",
    "                # From counterfactual back to original, class changes again\n",
    "                reconstructions[k] = self.generators[k](fakes[inv_k])\n",
    "                # Identites: class does not change\n",
    "                identities[k] = self.generators[k](batch)\n",
    "        if train:\n",
    "            return fakes, identities, reconstructions\n",
    "        return fakes\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        \"\"\"Get discriminator opinion on x\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict\n",
    "            classname -> images\n",
    "        \"\"\"\n",
    "        discrimination = {}\n",
    "        for k, batch in x.items():\n",
    "            discrimination[k] = self.discriminators[k](batch)\n",
    "        return discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3fa55",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cyclegan = CycleGAN(*classes)\n",
    "cyclegan.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91db612",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You will notice above that the `CycleGAN` takes an input in the form of a dictionary, but our datasets and data-loaders return the data in the form of two tensors. Below are two utility functions that will swap from data from one to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5d5ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility function to go to/from dictionaries/x,y tensors\n",
    "def get_as_xy(dictionary):\n",
    "    x = torch.cat([arr for arr in dictionary.values()])\n",
    "    y = []\n",
    "    for k, v in dictionary.items():\n",
    "        val = class_labels[k]\n",
    "        y += [\n",
    "            val,\n",
    "        ] * len(v)\n",
    "    y = torch.Tensor(y).to(x.device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_as_dictionary(x, y):\n",
    "    dictionary = {}\n",
    "    for k in classes:\n",
    "        val = class_to_idx[k]\n",
    "        # Get all of the indices for this class\n",
    "        this_class_indices = torch.where(y == val)\n",
    "        dictionary[k] = x[this_class_indices]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48e4af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Creating a training loop\n",
    "\n",
    "Now that we have a model, our next task is to create a training loop for the CycleGAN. This is a bit more difficult than the training loop for our classifier.\n",
    "\n",
    "Here are some of the things to keep in mind during the next task.\n",
    "\n",
    "1. The CycleGAN is (obviously) a GAN: a Generative Adversarial Network. What makes an adversarial network \"adversarial\" is that two different networks are working against each other. The loss that is used to optimize this is in our exercise `criterionGAN`. Although the specifics of this loss is beyond the score of this notebook, the idea is simple: the `criterionGAN` compares the output of the discriminator to a boolean-valued target. If we want the discriminator to think that it has seen a real image, we set the target to `True`. If we want the  discriminator to think that it has seen a generated image, we set the target to `False`. Note that it isn't important here whether the image *is* real, but **whether we want the discriminator to think it is real at that point**. (This will be important very soon 😉)\n",
    "\n",
    "2. Since the two networks are fighting each other, it is important to make sure that neither of them can cheat with information espionage. The CycleGAN implementation below is a turn-by-turn fight: we train the generator(s) and the discriminator(s) in alternating steps. When a model is not training, we will restrict its access to information by using `set_requries_grad` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482184f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cycle_gan.util.image_pool import ImagePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14194",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterionIdt = nn.L1Loss()\n",
    "criterionCycle = nn.L1Loss()\n",
    "criterionGAN = GANLoss(\"lsgan\")\n",
    "criterionGAN.to(device)\n",
    "\n",
    "lambda_idt = 1\n",
    "pool_size = 32\n",
    "\n",
    "lambdas = {k: 1 for k in classes}\n",
    "image_pools = {classname: ImagePool(pool_size) for classname in classes}\n",
    "\n",
    "optimizer_g = torch.optim.Adam(cyclegan.generators.parameters(), lr=1e-4)\n",
    "optimizer_d = torch.optim.Adam(cyclegan.discriminators.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a5f18",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 3.1: Set up the training losses and gradients</h4>\n",
    "\n",
    "In the code below, there are several spots with multiple options. Choose from among these, and delete or comment out the incorrect option.\n",
    "1. In `generator_step`: Choose whether the target to the`criterionGAN` should be `True` or `False`.\n",
    "2. In `discriminator_step`: Choose the target to the `criterionGAN` (note that there are two this time, one for the real images and one for the generated images)\n",
    "3. In `train_gan`: `set_requires_grad` correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36c59f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_requires_grad(module, value=True):\n",
    "    \"\"\"Sets `requires_grad` on a `module`'s parameters to `value`\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = value\n",
    "\n",
    "\n",
    "def generator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for generators G_X and G_Y\"\"\"\n",
    "    # Get all generated images\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    # Get discriminator opinion\n",
    "    discrimination = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        # Identity loss\n",
    "        # G_A should be identity if real_B is fed: ||G_A(B) - B||\n",
    "        loss_idt = criterionIdt(identities[k], reals[k]) * lambdas[k] * lambda_idt\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        #################### TODO Choice 1 #####################\n",
    "        # OPTION 1\n",
    "        # loss_G = criterionGAN(discrimination[k], False)\n",
    "        # OPTION2\n",
    "        # loss_G = criterionGAN(discrimination[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        # Forward cycle loss || G_B(G_A(A)) - A||\n",
    "        loss_cycle = criterionCycle(reconstructions[k], reals[k]) * lambdas[k]\n",
    "        # combined loss and calculate gradients\n",
    "        loss += loss_G + loss_cycle + loss_idt\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def discriminator_step(cyclegan, reals):\n",
    "    \"\"\"Calculate the loss for the discriminators D_X and D_Y\"\"\"\n",
    "    fakes, identities, reconstructions = cyclegan(reals)\n",
    "    preds_real = cyclegan.discriminate(reals)\n",
    "    # Get fakes from pool\n",
    "    fakes = {k: v.detach() for k, v in fakes.items()}\n",
    "    preds_fake = cyclegan.discriminate(fakes)\n",
    "    loss = 0\n",
    "    for k in classes:\n",
    "        #################### TODO Choice 2 #####################\n",
    "        # OPTION 1\n",
    "        # loss_real = criterionGAN(preds_real[k], True)\n",
    "        # loss_fake = criterionGAN(preds_fake[k], False)\n",
    "        # OPTION 2\n",
    "        # loss_real = criterionGAN(preds_real[k], False)\n",
    "        # loss_fake = criterionGAN(preds_fake[k], True)\n",
    "        #########################################################\n",
    "\n",
    "        loss += (loss_real + loss_fake) * 0.5\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def train_gan(reals):\n",
    "    \"\"\"Optimize the network parameters on a batch of images.\n",
    "\n",
    "    reals: Dict[str, torch.Tensor]\n",
    "        Classname -> Tensor dictionary of images.\n",
    "    \"\"\"\n",
    "    #################### TODO Choice 3 #####################\n",
    "    # OPTION 1\n",
    "    # set_requires_grad(cyclegan.generators, True)\n",
    "    # set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    # set_requires_grad(cyclegan.generators, False)\n",
    "    # set_requires_grad(cyclegan.discriminators, True)\n",
    "    ##########################################################\n",
    "\n",
    "    optimizer_g.zero_grad()\n",
    "    generator_step(cyclegan, reals)\n",
    "    optimizer_g.step()\n",
    "\n",
    "    #################### TODO (still) choice 3 #####################\n",
    "    # OPTION 1\n",
    "    # set_requires_grad(cyclegan.generators, True)\n",
    "    # set_requires_grad(cyclegan.discriminators, False)\n",
    "    # OPTION 2\n",
    "    # set_requires_grad(cyclegan.generators, False)\n",
    "    # set_requires_grad(cyclegan.discriminators, True)\n",
    "    #################################################################\n",
    "\n",
    "    optimizer_d.zero_grad()\n",
    "    discriminator_step(cyclegan, reals)\n",
    "    optimizer_d.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b90f36",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's add a quick plotting function before we begin training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2d5a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gan_output(sample=None):\n",
    "    # Get the input from the test dataset\n",
    "    if sample is None:\n",
    "        i = np.random.randint(len(gan_test_dataset))\n",
    "        x, y = gan_test_dataset[i]\n",
    "        x = x.to(device)\n",
    "        reals = {classes[y]: x}\n",
    "    else:\n",
    "        reals = sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fakes, identities, reconstructions = cyclegan(reals)\n",
    "    inverse_keys = cyclegan.inverse_keys\n",
    "    for k in reals.keys():\n",
    "        inv_k = inverse_keys[k]\n",
    "        for i in range(len(reals[k])):\n",
    "            fig, (ax, ax_fake, ax_id, ax_recon) = plt.subplots(1, 4)\n",
    "            ax.imshow(reals[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_fake.imshow(fakes[inv_k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_id.imshow(identities[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax_recon.imshow(reconstructions[k][i].squeeze().cpu(), cmap=\"gray\")\n",
    "            # Name the axes\n",
    "            ax.set_title(f\"{k.capitalize()}\")\n",
    "            ax_fake.set_title(\"Counterfactual\")\n",
    "            ax_id.set_title(\"Identity\")\n",
    "            ax_recon.set_title(\"Reconstruction\")\n",
    "            for ax in [ax, ax_fake, ax_id, ax_recon]:\n",
    "                ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519aba30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-info\"><h4>Task 3.2: Training!</h4>\n",
    "Let's train the CycleGAN one batch a time, plotting the output every so often to see how it is getting on.\n",
    "\n",
    "While you watch the model train, consider whether you think it will be successful at generating counterfactuals in the number of steps we give it. What is the minimum number of iterations you think are needed for this to work, and how much time do yo uthink it will take?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f44ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a balanced sampler that only considers the two classes\n",
    "sampler = balanced_sampler(gan_train_dataset)\n",
    "dataloader = DataLoader(\n",
    "    gan_train_dataset, batch_size=8, drop_last=True, sampler=sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370994c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of iterations to train for (note: this is not *nearly* enough to get ideal results)\n",
    "iterations = 500\n",
    "# Determines how often to plot outputs to see how the network is doing. I recommend scaling your `print_every` to your `iterations`.\n",
    "# For example, if you're running `iterations=5` you can `print_every=1`, but `iterations=1000` and `print_every=1` will be a lot of prints.\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861dedd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    real = get_as_dictionary(x, y)\n",
    "    train_gan(real)\n",
    "    if i % print_every == 0:\n",
    "        cyclegan.eval()  # Set to eval to speed up the plotting\n",
    "        plot_gan_output(sample=real)\n",
    "        cyclegan.train()  # Set back to train!\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3f362",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "...this time again.\n",
    "\n",
    "<img src=\"assets/model_train.jpg\" alt=\"drawing\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee205dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 3</h2>\n",
    "You've now learned the basics of what makes up a CycleGAN, and details on how to perform adversarial training.\n",
    "The same method can be used to create a CycleGAN with different basic elements.\n",
    "For example, you can change the archictecture of the generators, or of the discriminator to better fit your data in the future.\n",
    "\n",
    "You know the drill... let us know on the exercise chat!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765089a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 4: Evaluating the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959c219",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## That was fun!... let's load a pre-trained model\n",
    "\n",
    "Training the CycleGAN takes a lot longer than the few iterations that we did above. Since we don't have that kind of time, we are going to load a pre-trained model (for reference, this pre-trained model was trained for 7 days...).\n",
    "\n",
    "To continue, interrupt the kernel and continue with the next one, which will just use one of the pretrained CycleGAN models for the synapse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd97600",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, classA, classB):\n",
    "    \"\"\"Load the pre-trained models from the path\"\"\"\n",
    "    directory = Path(path).expanduser() / f\"{classA}_{classB}\"\n",
    "    # Load generators\n",
    "    model.generators[classB].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_G_A.pth\")\n",
    "    )\n",
    "    model.generators[classA].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_G_B.pth\")\n",
    "    )\n",
    "    # Load discriminators\n",
    "    model.discriminators[classA].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_D_A.pth\")\n",
    "    )\n",
    "    model.discriminators[classB].load_state_dict(\n",
    "        torch.load(directory / \"latest_net_D_B.pth\")\n",
    "    )\n",
    "\n",
    "\n",
    "load_pretrained(cyclegan, \"./checkpoints/synapses/cycle_gan/\", *classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee456f57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's look at some examples. Can you pick up on the differences between original, the counter-factual, and the reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adc855",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_gan_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1b783",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We're going to apply the CycleGAN to our test dataset, and save the results to be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887b0da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(gan_test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7c1e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.io import imsave\n",
    "\n",
    "\n",
    "def unnormalize(image):\n",
    "    return ((0.5 * image + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_gan(dataloader, directory):\n",
    "    \"\"\"Run CycleGAN on a dataloader and save images to a directory.\"\"\"\n",
    "    directory = Path(directory)\n",
    "    inverse_keys = cyclegan.inverse_keys\n",
    "    cyclegan.eval()\n",
    "    batch_size = dataloader.batch_size\n",
    "    n_sample = 0\n",
    "    for batch, (x, y) in enumerate(tqdm(dataloader)):\n",
    "        reals = get_as_dictionary(x.to(device), y.to(device))\n",
    "        fakes, _, recons = cyclegan(reals)\n",
    "        for k in reals.keys():\n",
    "            inv_k = inverse_keys[k]\n",
    "            (directory / f\"real/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "            (directory / f\"reconstructed/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "            (directory / f\"counterfactual/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "            for i, (im_real, im_fake, im_recon) in enumerate(\n",
    "                zip(reals[k], fakes[inv_k], recons[k])\n",
    "            ):\n",
    "                # Save real synapse images\n",
    "                imsave(\n",
    "                    directory / f\"real/{k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_real.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Save fake synapse images\n",
    "                imsave(\n",
    "                    directory / f\"reconstructed/{k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_recon.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Save counterfactual synapse images\n",
    "                imsave(\n",
    "                    directory / f\"counterfactual/{k}/{k}_{inv_k}_{n_sample}.png\",\n",
    "                    unnormalize(im_fake.cpu().numpy().squeeze()),\n",
    "                )\n",
    "                # Count\n",
    "                n_sample += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bfcf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "apply_gan(dataloader, \"test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0e50e",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up the gpu's memory a bit to avoid Out-of-Memory errors\n",
    "cyclegan = cyclegan.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483af604",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Evaluating the GAN\n",
    "\n",
    "The first thing to find out is whether the CycleGAN is successfully converting the images from one neurotransmitter to another.\n",
    "We will do this by running the classifier that we trained earlier on generated data.\n",
    "\n",
    "The data were saved in a directory called `test_images`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59702f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(directory):\n",
    "    \"\"\"Create a dataset from a directory of images with the classes in the same order as the VGG's output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: str\n",
    "        The root directory of the images. It should contain sub-directories named after the classes, in which images are stored.\n",
    "    \"\"\"\n",
    "    # Make a dataset with the classes in the correct order\n",
    "    limited_classes = {k: v for k, v in class_to_idx.items() if k in classes}\n",
    "    dataset = ImageFolder(root=directory, transform=transform)\n",
    "    samples = ImageFolder.make_dataset(\n",
    "        directory, class_to_idx=limited_classes, extensions=\".png\"\n",
    "    )\n",
    "    # Sort samples by name\n",
    "    samples = sorted(samples, key=lambda s: s[0].split(\"_\")[-1])\n",
    "    dataset.classes = classes\n",
    "    dataset.class_to_idx = limited_classes\n",
    "    dataset.samples = samples\n",
    "    dataset.targets = [s[1] for s in samples]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bffc67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 4.1 Get the classifier accuracy on CycleGAN outputs</h3>\n",
    "\n",
    "Using the saved images, we're going to figure out how good our CycleGAN is at generating images of a new class!\n",
    "\n",
    "The images (`real`, `reconstructed`, and `counterfactual`) are saved in the `test_images/` directory. Before you start the exercise, have a look at how this directory is organized.\n",
    "\n",
    "TODO\n",
    "- Use the `make_dataset` function to create a dataset for the three different image types that we saved above\n",
    "    - real\n",
    "    - reconstructed\n",
    "    - counterfactual\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42906ce7",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset of real images\n",
    "ds_real = ...\n",
    "# Dataset of reconstructed images (full cycle)\n",
    "ds_recon = ...\n",
    "# Datset of counterfactuals (half-cycle)\n",
    "ds_counterfactual = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4500183",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "We get the following accuracies:\n",
    "\n",
    "1. `accuracy_real`: Accuracy of the classifier on the real images, just for the two classes used in the GAN\n",
    "2. `accuracy_recon`: Accuracy of the classifier on the reconstruction.\n",
    "3. `accuracy_counter`: Accuracy of the classifier on the counterfactual images.\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "- In a perfect world, what value would we expect for `accuracy_recon`? What do we compare it to and why is it higher/lower?\n",
    "- How well is it translating from one class to another? Do we expect `accuracy_counter` to be large or small? Do we want it to be large or small? Why?\n",
    "\n",
    "Let us know your insights on the exercise chat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2af0c",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cf_pred, cf_gt = predict(ds_counterfactual, \"Counterfactuals\")\n",
    "recon_pred, recon_gt = predict(ds_recon, \"Reconstructions\")\n",
    "real_pred, real_gt = predict(ds_real, \"Real images\")\n",
    "\n",
    "# Get the accuracies\n",
    "accuracy_real = accuracy_score(real_gt, real_pred)\n",
    "accuracy_recon = accuracy_score(recon_gt, recon_pred)\n",
    "accuracy_cf = accuracy_score(cf_gt, cf_pred)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy real: {accuracy_real}\\nAccuracy reconstruction: {accuracy_recon}\\nAccuracy counterfactuals: {accuracy_cf}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c9449",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We're going to look at the confusion matrices for the counterfactuals, and compare it to that of the real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e1278",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [class_to_idx[i] for i in classes]\n",
    "print(\"The confusion matrix of the classifier on the counterfactuals\")\n",
    "cm_analysis(cf_pred, cf_gt, names=classes, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92401b45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The confusion matrix on the real images... for comparison\")\n",
    "cm_analysis(real_pred, real_gt, names=classes, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8cca6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "<h3>Questions</h3>\n",
    "\n",
    "- What would you expect the confusion matrix for the counterfactuals to look like? Why?\n",
    "- Do the two directions of the CycleGAN work equally as well?\n",
    "- Can you think of anything that might have made it more difficult, or easier, to translate in a one direction vs the other?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bbc95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 4</h2>\n",
    " We have seen that our CycleGAN network has successfully translated some of the synapses from one class to the other, but there are clearly some things to look out for!\n",
    "Take the time to think about the questions above before moving on...\n",
    "\n",
    "This is the end of Section 4. Let us know on the exercise chat if you have reached this point!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e8777",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 5: Highlighting Class-Relevant Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee980b",
   "metadata": {},
   "source": [
    "At this point we have:\n",
    "- A classifier that can differentiate between neurotransmitters from EM images of synapses\n",
    "- A vague idea of which parts of the images it thinks are important for this classification\n",
    "- A CycleGAN that is sometimes able to trick the classifier with barely perceptible changes\n",
    "\n",
    "What we don't know, is *how* the CycleGAN is modifying the images to change their class.\n",
    "\n",
    "To start to answer this question, we will use a [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412) method to highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbe347",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 5.1 Get sucessfully converted samples</h3>\n",
    "The CycleGAN is able to convert some, but not all images into their target types.\n",
    "In order to observe and highlight useful differences, we want to observe our attribution method at work only on those examples of synapses:\n",
    "<ol>\n",
    "    <li> That were correctly classified originally</li>\n",
    "    <li>Whose counterfactuals were also correctly classified</li>\n",
    "</ol>\n",
    "\n",
    "TODO\n",
    "- Get a boolean description of the `real` samples that were correctly predicted\n",
    "- Get the target class for the `counterfactual` images (Hint: It isn't `cf_gt`!)\n",
    "- Get a boolean description of the `cf` samples that have the target class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec78be",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### Task 5.1 TODO #######\n",
    "\n",
    "# Get the samples where the real is correct\n",
    "correct_real = ...\n",
    "\n",
    "# HINT GABA is class 1 and ACh is class 0\n",
    "target = ...\n",
    "\n",
    "# Get the samples where the counterfactual has reached the target\n",
    "correct_cf = ...\n",
    "\n",
    "# Successful conversions\n",
    "success = np.where(np.logical_and(correct_real, correct_cf))[0]\n",
    "\n",
    "# Create datasets with only the successes\n",
    "cf_success_ds = Subset(ds_counterfactual, success)\n",
    "real_success_ds = Subset(ds_real, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518deea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To check that we have got it right, let us get the accuracy on the best 100 vs the worst 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813f006",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599f126",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_true, real_pred = predict(real_success_ds, \"Real\")\n",
    "cf_true, cf_pred = predict(cf_success_ds, \"Counterfactuals\")\n",
    "\n",
    "print(\n",
    "    \"Accuracy of the classifier on successful real images\",\n",
    "    accuracy_score(real_true, real_pred),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of the classifier on successful counterfactual images\",\n",
    "    accuracy_score(cf_true, cf_pred),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877db1dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Creating hybrids from attributions\n",
    "\n",
    "Now that we have a set of successfully translated counterfactuals, we can use them as a baseline for our attribution.\n",
    "If you remember from earlier, `IntegratedGradients` does a interpolation between the model gradients at the baseline and the model gradients at the sample. Here, we're also going to be doing an interpolation between the baseline image and the sample image, creating a hybrid!\n",
    "\n",
    "To do this, we will take the sample image and mask out all of the pixels in the attribution. We will then replace these masked out pixels by the equivalent values in the counterfactual. So we'll have a hybrid image that is like the original everywhere except in the areas that matter for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7288f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_real = DataLoader(real_success_ds, batch_size=10)\n",
    "dataloader_counter = DataLoader(cf_success_ds, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95239b4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    # Create an integrated gradients object.\n",
    "    # integrated_gradients = IntegratedGradients(model)\n",
    "    # Generated attributions on integrated gradients\n",
    "    attributions = np.vstack(\n",
    "        [\n",
    "            integrated_gradients.attribute(\n",
    "                real.to(device),\n",
    "                target=target.to(device),\n",
    "                baselines=counterfactual.to(device),\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            for (real, target), (counterfactual, _) in zip(\n",
    "                dataloader_real, dataloader_counter\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b968d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84835390",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for creating an interactive visualization of our attributions\n",
    "model.cpu()\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"viridis\")\n",
    "colors = cmap([0, 255])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_classifications(image, counter, hybrid):\n",
    "    model.eval()\n",
    "    class_idx = [full_dataset.classes.index(c) for c in classes]\n",
    "    tensor = torch.from_numpy(np.stack([image, counter, hybrid])).float()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)[:, class_idx]\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred, counter_pred, hybrid_pred = probs\n",
    "    return pred.numpy(), counter_pred.numpy(), hybrid_pred.numpy()\n",
    "\n",
    "\n",
    "def visualize_counterfactuals(idx, threshold=0.1):\n",
    "    image = real_success_ds[idx][0].numpy()\n",
    "    counter = cf_success_ds[idx][0].numpy()\n",
    "    mask = get_mask(attributions[idx], threshold)\n",
    "    hybrid = (1 - mask) * image + mask * counter\n",
    "    nan_mask = copy.deepcopy(mask)\n",
    "    nan_mask[nan_mask != 0] = 1\n",
    "    nan_mask[nan_mask == 0] = np.nan\n",
    "    # PLOT\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"\"\"\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   ....ggg.fff.ppp\n",
    "                                   \"\"\",\n",
    "        figsize=(20, 5),\n",
    "    )\n",
    "    # Original\n",
    "    viz.visualize_image_attr(\n",
    "        np.transpose(mask, (1, 2, 0)),\n",
    "        np.transpose(image, (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"absolute_value\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Mask\",\n",
    "        use_pyplot=False,\n",
    "        plt_fig_axis=(fig, axes[\"m\"]),\n",
    "    )\n",
    "    # Original\n",
    "    axes[\"o\"].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axes[\"o\"].set_title(\"Original\", fontsize=24)\n",
    "    # Counterfactual\n",
    "    axes[\"c\"].imshow(counter.squeeze(), cmap=\"gray\")\n",
    "    axes[\"c\"].set_title(\"Counterfactual\", fontsize=24)\n",
    "    # Hybrid\n",
    "    axes[\"h\"].imshow(hybrid.squeeze(), cmap=\"gray\")\n",
    "    axes[\"h\"].set_title(\"Hybrid\", fontsize=24)\n",
    "    # Mask\n",
    "    pred, counter_pred, hybrid_pred = get_classifications(image, counter, hybrid)\n",
    "    axes[\"g\"].barh(classes, pred, color=colors)\n",
    "    axes[\"f\"].barh(classes, counter_pred, color=colors)\n",
    "    axes[\"p\"].barh(classes, hybrid_pred, color=colors)\n",
    "    for ix in [\"m\", \"o\", \"c\", \"h\"]:\n",
    "        axes[ix].axis(\"off\")\n",
    "\n",
    "    for ix in [\"g\", \"f\", \"p\"]:\n",
    "        for tick in axes[ix].get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "        axes[ix].set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732d7a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 5.2: Observing the effect of the changes on the classifier</h3>\n",
    "Below is a small widget to interact with the above analysis. As you change the `threshold`, see how the prediction of the hybrid changes.\n",
    "At what point does it swap over?\n",
    "\n",
    "If you want to see different samples, slide through the `idx`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23225866",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interact(visualize_counterfactuals, idx=(0, 99), threshold=(0.0, 1.0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca835c5",
   "metadata": {},
   "source": [
    "HELP!!! Interactive (still!) doesn't work. No worries... uncomment the following cell and choose your index and threshold by typing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fb28f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your own adventure\n",
    "# idx = 0\n",
    "# threshold = 0.1\n",
    "\n",
    "# # Plotting :)\n",
    "# visualize_counterfactuals(idx, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905e9a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>Questions</h4>\n",
    "\n",
    "- Can you find features that define either of the two classes?\n",
    "-  How consistent are they across the samples?\n",
    "-  Is there a range of thresholds where most of the hybrids swap over to the target class? (If you want to see that area, try to change the range of thresholds in the slider by setting `threshold=(minimum_value, maximum_value, step_size)`\n",
    "\n",
    "Feel free to discuss your answers on the exercise chat!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e5831",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1>The End.</h1>\n",
    "    Go forth and train some GANs!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8cb30e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Going Further\n",
    "\n",
    "Here are some ideas for how to continue with this notebook:\n",
    "\n",
    "1. Improve the classifier. This code uses a VGG network for the classification. On the synapse dataset, we will get a validation accuracy of around 80%. Try to see if you can improve the classifier accuracy.\n",
    "    * (easy) Data augmentation: The training code for the classifier is quite simple in this example. Enlarge the amount of available training data by adding augmentations (transpose and mirror the images, add noise, change the intensity, etc.).\n",
    "    * (easy) Network architecture: The VGG network has a few parameters that one can tune. Try a few to see what difference it makes.\n",
    "    * (easy) Inspect the classifier predictions: Take random samples from the test dataset and classify them. Show the images together with their predicted and actual labels.\n",
    "    * (medium) Other networks:  Try different architectures (e.g., a [ResNet](https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/#resnet-from-scratch)) and see if the accuracy can be improved.\n",
    "\n",
    "2. Explore the CycleGAN.\n",
    "    * (easy) The example code below shows how to translate between GABA and acetylcholine. Try different combinations. Can you start to see differences between some pairs of classes? Which are the ones where the differences are the most or the least obvious? Can you see any differences that aren't well described by the mask? How would you describe these?\n",
    "\n",
    "3. Try on your own data!\n",
    "    * Have a look at how the synapse images are organized in `data/raw/synapses`. Copy the directory structure and use your own images. Depending on your data, you might have to adjust the image size (128x128 for the synapses) and number of channels in the VGG network and CycleGAN code."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "09_knowledge_extraction",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
