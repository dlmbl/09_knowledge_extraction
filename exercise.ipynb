{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ddb066",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "# Exercise 8: Knowledge Extraction from a Pre-trained Neural Network\n",
    "\n",
    "The goal of this exercise is to learn how to probe what a pre-trained classifier has learned about the data it was trained on.\n",
    "\n",
    "We will be working with a simple example which is a fun derivation on the MNIST dataset that you will have seen in previous exercises in this course.\n",
    "Unlike regular MNIST, our dataset is classified not by number, but by color!\n",
    "\n",
    "We will:\n",
    "1. Load a pre-trained classifier and try applying conventional attribution methods\n",
    "2. Train a GAN to create counterfactual images - translating images from one class to another\n",
    "3. Evaluate the GAN - see how good it is at fooling the classifier\n",
    "4. Create attributions from the counterfactual, and learn the differences between the classes.\n",
    "\n",
    "If time permits, we will try to apply this all over again as a bonus exercise to a much more complex and more biologically relevant problem.\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook was written by Diane Adjavon, from a previous version written by Jan Funke and modified by Tri Nguyen, using code from Nils Eckstein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43c1e3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>08_knowledge_extraction</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aaf840",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Setup\n",
    "\n",
    "In this part of the notebook, we will load the same dataset as in the previous exercise.\n",
    "We will also learn to load one of our trained classifiers from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c5bc0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "from classifier.data import ColoredMNIST\n",
    "\n",
    "mnist = ColoredMNIST(\"data\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd19fe5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Some information about the dataset:\n",
    "- The dataset is a colored version of the MNIST dataset.\n",
    "- Instead of using the digits as classes, we use the colors.\n",
    "- There are four classes - the goal of the exercise is to find out what these are.\n",
    "\n",
    "Let's plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c709838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    x, y = mnist[i]\n",
    "    x = x.permute((1, 2, 0))  # make channels last\n",
    "    ax.imshow(x)\n",
    "    ax.set_title(f\"Class {y}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04f969",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We have pre-traiend a classifier for you on this dataset. It is the same architecture classifier as you used in the Failure Modes exercise: a `DenseModel`.\n",
    "Let's load that classifier now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea9906",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1.1: Load the classifier</h3>\n",
    "We have written a slightly more general version of the `DenseModel` that you used in the previous exercise. Ours requires two inputs:\n",
    "- `input_shape`: the shape of the input images, as a tuple\n",
    "- `num_classes`: the number of classes in the dataset\n",
    "\n",
    "Create a dense model with the right inputs and load the weights from the checkpoint.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf64cda",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from classifier.model import DenseModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO Load the model with the correct input shape\n",
    "model = DenseModel(input_shape=(...), num_classes=4)\n",
    "\n",
    "# TODO modify this with the location of your classifier checkpoint\n",
    "checkpoint = torch.load(...)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad014ac",
   "metadata": {},
   "source": [
    "# Part 2: Using Integrated Gradients to find what the classifier knows\n",
    "\n",
    "In this section we will make a first attempt at highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40eeba7",
   "metadata": {},
   "source": [
    "## Attributions through integrated gradients\n",
    "\n",
    "Attribution is the process of finding out, based on the output of a neural network, which pixels in the input are (most) responsible. Another way of thinking about it is: which pixels would need to change in order for the network's output to change.\n",
    "\n",
    "Here we will look at an example of an attribution method called [Integrated Gradients](https://captum.ai/docs/extension/integrated_gradients). If you have a bit of time, have a look at this [super fun exploration of attribution methods](https://distill.pub/2020/attribution-baselines/), especially the explanations on Integrated Gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aca710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = [mnist[i] for i in range(batch_size)]\n",
    "x = torch.stack([b[0] for b in batch])\n",
    "y = torch.tensor([b[1] for b in batch])\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d286aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.1 Get an attribution</h3>\n",
    "\n",
    "In this next part, we will get attributions on single batch. We use a library called [captum](https://captum.ai), and focus on the `IntegratedGradients` method.\n",
    "Create an `IntegratedGradients` object and run attribution on `x,y` obtained above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec387f82",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "############### Task 2.1 TODO ############\n",
    "# Create an integrated gradients object.\n",
    "integrated_gradients = ...\n",
    "\n",
    "# Generated attributions on integrated gradients\n",
    "attributions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea56240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributions = (\n",
    "    attributions.cpu().numpy()\n",
    ")  # Move the attributions from the GPU to the CPU, and turn then into numpy arrays for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50850e",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "Here is an example for an image, and its corresponding attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7447933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_attribution(attribution, original_image):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "\n",
    "    viz.visualize_image_attr_multiple(\n",
    "        attribution,\n",
    "        original_image,\n",
    "        methods=[\"original_image\", \"heat_map\"],\n",
    "        signs=[\"all\", \"absolute_value\"],\n",
    "        show_colorbar=True,\n",
    "        titles=[\"Image\", \"Attribution\"],\n",
    "        use_pyplot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb527bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecec3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "The attributions are shown as a heatmap. The brighter the pixel, the more important this attribution method thinks that it is.\n",
    "As you can see, it is pretty good at recognizing the number within the image.\n",
    "As we know, however, it is not the digit itself that is important for the classification, it is the color!\n",
    "Although the method is picking up really well on the region of interest, it would be difficult to conclude from this that it is the color that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43f05d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Something is slightly unfair about this visualization though.\n",
    "We are visualizing as if it were grayscale, but both our images and our attributions are in color!\n",
    "Can we learn more from the attributions if we visualize them in color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b21894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_color_attribution(attribution, original_image):\n",
    "    attribution = np.transpose(attribution, (1, 2, 0))\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(original_image)\n",
    "    ax1.set_title(\"Image\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(np.abs(attribution))\n",
    "    ax2.set_title(\"Attribution\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for attr, im in zip(attributions, x.cpu().numpy()):\n",
    "    visualize_color_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97eace2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We get some better clues when looking at the attributions in color.\n",
    "The highlighting doesn't just happen in the region with number, but also seems to hapen in a channel that matches the color of the image.\n",
    "Just based on this, however, we don't get much more information than we got from the images themselves.\n",
    "\n",
    "If we didn't know in advance, it is unclear whether the color or the number is the most important feature for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b7d6e",
   "metadata": {},
   "source": [
    "\n",
    "### Changing the basline\n",
    "\n",
    "Many existing attribution algorithms are comparative: they show which pixels of the input are responsible for a network output *compared to a baseline*.\n",
    "The baseline is often set to an all 0 tensor, but the choice of the baseline affects the output.\n",
    "(For an interactive illustration of how the baseline affects the output, see [this Distill paper](https://distill.pub/2020/attribution-baselines/))\n",
    "\n",
    "You can change the baseline used by the `integrated_gradients` object.\n",
    "\n",
    "Use the command:\n",
    "```\n",
    "?integrated_gradients.attribute\n",
    "```\n",
    "To get more details about how to include the baseline.\n",
    "\n",
    "Try using the code above to change the baseline and see how this affects the output.\n",
    "\n",
    "1. Random noise as a baseline\n",
    "2. A blurred/noisy version of the original image as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13ae8d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.3: Use random noise as a baseline</h4>\n",
    "\n",
    "Hint: `torch.rand_like`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6691c",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "random_baselines = ...  # TODO Change\n",
    "# Generate the attributions\n",
    "attributions_random = integrated_gradients.attribute(...)  # TODO Change\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_random.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c00a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h4>Task 2.4: Use a blurred image a baseline</h4>\n",
    "\n",
    "Hint: `torchvision.transforms.functional` has a useful function for this ;)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835de1a",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO Import required function\n",
    "\n",
    "# Baseline\n",
    "blurred_baselines = ...  # TODO Create blurred version of the images\n",
    "# Generate the attributions\n",
    "attributions_blurred = integrated_gradients.attribute(...)  # TODO Fill\n",
    "\n",
    "# Plotting\n",
    "for attr, im in zip(attributions_blurred.cpu().numpy(), x.cpu().numpy()):\n",
    "    visualize_color_attribution(attr, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a6cfcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"altert alert-block alert-warning\"><h4> Questions </h4>\n",
    "<ul>\n",
    "<li>What baseline do you like best so far? Why?</li>\n",
    "<li>Why do you think some baselines work better than others?</li>\n",
    "<li>If you were to design an ideal baseline, what would you choose?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3d08e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h2>BONUS Task: Using different attributions.</h2>\n",
    "\n",
    "\n",
    "[`captum`](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) has access to various different attribution algorithms.\n",
    "\n",
    "Replace `IntegratedGradients` with different attribution methods. Are they consistent with each other?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d946a8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 2</h2>\n",
    "Let us know on the exercise chat when you've reached this point!\n",
    "\n",
    "At this point we have:\n",
    "\n",
    "- Loaded a classifier that classifies MNIST-like images by color, but we don't know how!\n",
    "- Tried applying Integrated Gradients to find out what the classifier is looking at - with little success.\n",
    "- Discovered the effect of changing the baseline on the output of integrated gradients.\n",
    "\n",
    "Coming up in the next section, we will learn how to create counterfactual images.\n",
    "These images will change *only what is necessary* in order to change the classification of the image.\n",
    "We'll see that using counterfactuals we will be able to disambiguate between color and number as an important feature.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04602cf9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 3: Train a GAN to Translate Images\n",
    "\n",
    "To gain insight into how the trained network classify images, we will use [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412), a feature attribution with counterfactual explanations methodology.\n",
    "This method employs a StarGAN to translate images from one class to another to make counterfactual explanations.\n",
    "\n",
    "**What is a counterfactual?**\n",
    "\n",
    "You've learned about adversarial examples in the lecture on failure modes. These are the imperceptible or noisy changes to an image that drastically changes a classifier's opinion.\n",
    "Counterfactual explanations are the useful cousins of adversarial examples. They are *perceptible* and *informative* changes to an image that changes a classifier's opinion.\n",
    "\n",
    "In the image below you can see the difference between the two. In the first column are MNIST images along with their classifictaions, and in the second column are counterfactual explanations to *change* that class. You can see that in both cases a human being would (hopefully) agree with the new classification. By comparing the two columns, we can therefore begin to define what makes each digit special.\n",
    "\n",
    "In contrast, the third and fourth columns show an MNIST image and a corresponding adversarial example. Here the network returns a prediction that most human beings (who aren't being facetious) would strongly disagree with.\n",
    "\n",
    "<img src=\"assets/ce_vs_ae.png\" width=50% />\n",
    "\n",
    "**Counterfactual synapses**\n",
    "\n",
    "In this example, we will train a StarGAN network that is able to take any of our special MNIST images and change its class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed173d7c",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "### The model\n",
    "![stargan.png](assets/stargan.png)\n",
    "\n",
    "In the following, we create a [StarGAN model](https://arxiv.org/abs/1711.09020).\n",
    "It is a Generative Adversarial model that is trained to turn one class of images X into a different class of images Y.\n",
    "\n",
    "We will not be using the random latent code (green, in the figure), so the model we use is made up of three networks:\n",
    "- The generator - this will be the bulk of the model, and will be responsible for transforming the images: we're going to use a `UNet`\n",
    "- The discriminator - this will be responsible for telling the difference between real and fake images: we're going to use a `DenseModel`\n",
    "- The style encoder - this will be responsible for encoding the style of the image: we're going to use a `DenseModel`\n",
    "\n",
    "Let's start by creating these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a51bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlmbl_unet import UNet\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, generator, style_encoder):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.style_encoder = style_encoder\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor\n",
    "            The source image\n",
    "        y: torch.Tensor\n",
    "            The style image\n",
    "        \"\"\"\n",
    "        style = self.style_encoder(y)\n",
    "        # Concatenate the style vector with the input image\n",
    "        style = style.unsqueeze(-1).unsqueeze(-1)\n",
    "        style = style.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, style], dim=1)\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6168d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3.1: Create the models</h3>\n",
    "\n",
    "We are going to create the models for the generator, discriminator, and style mapping.\n",
    "\n",
    "Given the Generator structure above, fill in the missing parts for the unet and the style mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ef49f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "style_size = ...  # TODO choose a size for the style space\n",
    "unet_depth = ...  # TODO Choose a depth for the UNet\n",
    "style_mapping = DenseModel(\n",
    "    input_shape=..., num_classes=...  # How big is the style space?\n",
    ")\n",
    "unet = UNet(depth=..., in_channels=..., out_channels=..., final_activation=nn.Sigmoid())\n",
    "\n",
    "generator = Generator(unet, style_mapping=style_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd761ef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>Hyper-parameter choices</h3>\n",
    "<ul>\n",
    "<li>Are any of the hyperparameters you choose above constrained in some way?</li>\n",
    "<li>What would happen if you chose a depth of 10 for the UNet?</li>\n",
    "<li>Is there a minimum size for the style space? Why or why not?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1220bb6",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3.2: Create the discriminator</h3>\n",
    "\n",
    "We want the discriminator to be like a classifier, so it is able to look at an image and tell not only whether it is real, but also which class it came from.\n",
    "The discriminator will take as input either a real image or a fake image.\n",
    "Fill in the following code to create a discriminator that can classify the images into the correct number of classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71482197",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator = DenseModel(input_shape=..., num_classes=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709affba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's move all models onto the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7581c",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "## Training a GAN\n",
    "\n",
    "Training an adversarial network is a bit more complicated than training a classifier.\n",
    "For starters, we are simultaneously training two different networks that work against each other.\n",
    "As such, we need to be careful about how and when we update the weights of each network.\n",
    "\n",
    "We will have two different optimizers, one for the Generator and one for the Discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805887e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad28d8",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "\n",
    "There are also two different types of losses that we will need.\n",
    "**Adversarial loss**\n",
    "This loss describes how well the discriminator can tell the difference between real and generated images.\n",
    "In our case, this will be a sort of classification loss - we will use Cross Entropy.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "The adversarial loss will be applied differently to the generator and the discriminator! Be very careful!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverial_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c590737",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**Cycle/reconstruction loss**\n",
    "The cycle loss is there to make sure that the generator doesn't output an image that looks nothing like the input!\n",
    "Indeed, by training the generator to be able to cycle back to the original image, we are making sure that it makes a minimum number of changes.\n",
    "The cycle loss is applied only to the generator.\n",
    "\n",
    "cycle_loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def44d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0c1d2e",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-info\"><h4>Task 3.2: Training!</h4>\n",
    "Let's train the CycleGAN one batch a time, plotting the output every so often to see how it is getting on.\n",
    "\n",
    "While you watch the model train, consider whether you think it will be successful at generating counterfactuals in the number of steps we give it. What is the minimum number of iterations you think are needed for this to work, and how much time do yo uthink it will take?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f577571",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "...this time again.\n",
    "\n",
    "<img src=\"assets/model_train.jpg\" alt=\"drawing\" width=\"500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3077e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also turn this into a standalone script for use during the project phase\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def set_requires_grad(module, value=True):\n",
    "    \"\"\"Sets `requires_grad` on a `module`'s parameters to `value`\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = value\n",
    "\n",
    "\n",
    "cycle_loss_fn = nn.L1Loss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=1e-6)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    mnist, batch_size=32, drop_last=True, shuffle=True\n",
    ")  # We will use the same dataset as before\n",
    "\n",
    "losses = {\"cycle\": [], \"adv\": [], \"disc\": []}\n",
    "for epoch in range(50):\n",
    "    for x, y in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # get the target y by shuffling the classes\n",
    "        # get the style sources by random sampling\n",
    "        random_index = torch.randperm(len(y))\n",
    "        x_style = x[random_index].clone()\n",
    "        y_target = y[random_index].clone()\n",
    "\n",
    "        set_requires_grad(generator, True)\n",
    "        set_requires_grad(discriminator, False)\n",
    "        optimizer_g.zero_grad()\n",
    "        # Get the fake image\n",
    "        x_fake = generator(x, x_style)\n",
    "        # Try to cycle back\n",
    "        x_cycled = generator(x_fake, x)\n",
    "        # Discriminate\n",
    "        discriminator_x_fake = discriminator(x_fake)\n",
    "        # Losses to  train the generator\n",
    "\n",
    "        # 1. make sure the image can be reconstructed\n",
    "        cycle_loss = cycle_loss_fn(x, x_cycled)\n",
    "        # 2. make sure the discriminator is fooled\n",
    "        adv_loss = class_loss_fn(discriminator_x_fake, y_target)\n",
    "\n",
    "        # Optimize the generator\n",
    "        (cycle_loss + adv_loss).backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        set_requires_grad(generator, False)\n",
    "        set_requires_grad(discriminator, True)\n",
    "        optimizer_d.zero_grad()\n",
    "        # TODO Do I need to re-do the forward pass?\n",
    "        discriminator_x = discriminator(x)\n",
    "        discriminator_x_fake = discriminator(x_fake.detach())\n",
    "        # Losses to train the discriminator\n",
    "        # 1. make sure the discriminator can tell real is real\n",
    "        real_loss = class_loss_fn(discriminator_x, y)\n",
    "        # 2. make sure the discriminator can't tell fake is fake\n",
    "        fake_loss = -class_loss_fn(discriminator_x_fake, y_target)\n",
    "        #\n",
    "        disc_loss = (real_loss + fake_loss) * 0.5\n",
    "        disc_loss.backward()\n",
    "        # Optimize the discriminator\n",
    "        optimizer_d.step()\n",
    "\n",
    "        losses[\"cycle\"].append(cycle_loss.item())\n",
    "        losses[\"adv\"].append(adv_loss.item())\n",
    "        losses[\"disc\"].append(disc_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232bd07",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[\"cycle\"], label=\"Cycle loss\")\n",
    "plt.plot(losses[\"adv\"], label=\"Adversarial loss\")\n",
    "plt.plot(losses[\"disc\"], label=\"Discriminator loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de7380",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's add a quick plotting function before we begin training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856af9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "axs[0].imshow(x[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[1].imshow(x_style[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[2].imshow(x_fake[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "axs[3].imshow(x_cycled[idx].cpu().permute(1, 2, 0).detach().numpy())\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# TODO WIP here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7240ca5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 3</h2>\n",
    "You've now learned the basics of what makes up a CycleGAN, and details on how to perform adversarial training.\n",
    "The same method can be used to create a CycleGAN with different basic elements.\n",
    "For example, you can change the archictecture of the generators, or of the discriminator to better fit your data in the future.\n",
    "\n",
    "You know the drill... let us know on the exercise chat!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67168867",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4: Evaluating the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdbfde",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## That was fun!... let's load a pre-trained model\n",
    "\n",
    "Training the CycleGAN takes a lot longer than the few iterations that we did above. Since we don't have that kind of time, we are going to load a pre-trained model (for reference, this pre-trained model was trained for 7 days...).\n",
    "\n",
    "To continue, interrupt the kernel and continue with the next one, which will just use one of the pretrained CycleGAN models for the synapse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8543304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# TODO load the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b48d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's look at some examples. Can you pick up on the differences between original, the counter-factual, and the reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9425d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO show some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f81f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "We're going to apply the GAN to our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbfc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO load the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ded88d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating the GAN\n",
    "\n",
    "The first thing to find out is whether the CycleGAN is successfully converting the images from one neurotransmitter to another.\n",
    "We will do this by running the classifier that we trained earlier on generated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7475dc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 4.1 Get the classifier accuracy on CycleGAN outputs</h3>\n",
    "\n",
    "Using the saved images, we're going to figure out how good our CycleGAN is at generating images of a new class!\n",
    "\n",
    "The images (`real`, `reconstructed`, and `counterfactual`) are saved in the `test_images/` directory. Before you start the exercise, have a look at how this directory is organized.\n",
    "\n",
    "TODO\n",
    "- Use the `make_dataset` function to create a dataset for the three different image types that we saved above\n",
    "    - real\n",
    "    - reconstructed\n",
    "    - counterfactual\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a88ddb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "We get the following accuracies:\n",
    "\n",
    "1. `accuracy_real`: Accuracy of the classifier on the real images, just for the two classes used in the GAN\n",
    "2. `accuracy_recon`: Accuracy of the classifier on the reconstruction.\n",
    "3. `accuracy_counter`: Accuracy of the classifier on the counterfactual images.\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "- In a perfect world, what value would we expect for `accuracy_recon`? What do we compare it to and why is it higher/lower?\n",
    "- How well is it translating from one class to another? Do we expect `accuracy_counter` to be large or small? Do we want it to be large or small? Why?\n",
    "\n",
    "Let us know your insights on the exercise chat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make a loop on the data that creates the counterfactual images, given a set of options as input\n",
    "counterfactuals, reconstructions, targets, labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93db0b2",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "[markwodn]"
   },
   "outputs": [],
   "source": [
    "# Evaluate the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ccc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use the loaded classifier to evaluate the images\n",
    "# Get the accuracies\n",
    "def predict():\n",
    "    # TODO return predictions, labels\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47955f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "We're going to look at the confusion matrices for the counterfactuals, and compare it to that of the real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94284732",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(\"The confusion matrix on the real images... for comparison\")\n",
    "# TODO Confusion matrix on the counterfactual images\n",
    "confusion_matrix = ...\n",
    "# TODO plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The confusion matrix on the real images... for comparison\")\n",
    "# TODO Confusion matrix on the real images, for comparison\n",
    "confusion_matrix = ...\n",
    "# TODO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba5707",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-banner alert-warning\">\n",
    "<h3>Questions</h3>\n",
    "\n",
    "- What would you expect the confusion matrix for the counterfactuals to look like? Why?\n",
    "- Do the two directions of the CycleGAN work equally as well?\n",
    "- Can you think of anything that might have made it more difficult, or easier, to translate in a one direction vs the other?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9713122",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 4</h2>\n",
    " We have seen that our CycleGAN network has successfully translated some of the synapses from one class to the other, but there are clearly some things to look out for!\n",
    "Take the time to think about the questions above before moving on...\n",
    "\n",
    "This is the end of Section 4. Let us know on the exercise chat if you have reached this point!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183344be",
   "metadata": {},
   "source": [
    "# Part 5: Highlighting Class-Relevant Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83417bff",
   "metadata": {},
   "source": [
    "At this point we have:\n",
    "- A classifier that can differentiate between neurotransmitters from EM images of synapses\n",
    "- A vague idea of which parts of the images it thinks are important for this classification\n",
    "- A CycleGAN that is sometimes able to trick the classifier with barely perceptible changes\n",
    "\n",
    "What we don't know, is *how* the CycleGAN is modifying the images to change their class.\n",
    "\n",
    "To start to answer this question, we will use a [Discriminative Attribution from Counterfactuals](https://arxiv.org/abs/2109.13412) method to highlight differences between the \"real\" and \"fake\" images that are most important to change the decision of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ae577",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 5.1 Get sucessfully converted samples</h3>\n",
    "The CycleGAN is able to convert some, but not all images into their target types.\n",
    "In order to observe and highlight useful differences, we want to observe our attribution method at work only on those examples of synapses:\n",
    "<ol>\n",
    "    <li> That were correctly classified originally</li>\n",
    "    <li>Whose counterfactuals were also correctly classified</li>\n",
    "</ol>\n",
    "\n",
    "TODO\n",
    "- Get a boolean description of the `real` samples that were correctly predicted\n",
    "- Get the target class for the `counterfactual` images (Hint: It isn't `cf_gt`!)\n",
    "- Get a boolean description of the `cf` samples that have the target class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c56d18",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### Task 5.1 TODO #######\n",
    "\n",
    "# Get the samples where the real is correct\n",
    "correct_real = ...\n",
    "\n",
    "# HINT GABA is class 1 and ACh is class 0\n",
    "target = ...\n",
    "\n",
    "# Get the samples where the counterfactual has reached the target\n",
    "correct_cf = ...\n",
    "\n",
    "# Successful conversions\n",
    "success = np.where(np.logical_and(correct_real, correct_cf))[0]\n",
    "\n",
    "# Create datasets with only the successes\n",
    "cf_success_ds = Subset(ds_counterfactual, success)\n",
    "real_success_ds = Subset(ds_real, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737c833",
   "metadata": {
    "tags": []
   },
   "source": [
    "To check that we have got it right, let us get the accuracy on the best 100 vs the worst 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f6090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a0107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_true, real_pred = predict(real_success_ds, \"Real\")\n",
    "cf_true, cf_pred = predict(cf_success_ds, \"Counterfactuals\")\n",
    "\n",
    "print(\n",
    "    \"Accuracy of the classifier on successful real images\",\n",
    "    accuracy_score(real_true, real_pred),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of the classifier on successful counterfactual images\",\n",
    "    accuracy_score(cf_true, cf_pred),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edae8d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating hybrids from attributions\n",
    "\n",
    "Now that we have a set of successfully translated counterfactuals, we can use them as a baseline for our attribution.\n",
    "If you remember from earlier, `IntegratedGradients` does a interpolation between the model gradients at the baseline and the model gradients at the sample. Here, we're also going to be doing an interpolation between the baseline image and the sample image, creating a hybrid!\n",
    "\n",
    "To do this, we will take the sample image and mask out all of the pixels in the attribution. We will then replace these masked out pixels by the equivalent values in the counterfactual. So we'll have a hybrid image that is like the original everywhere except in the areas that matter for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d46ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_real = DataLoader(real_success_ds, batch_size=10)\n",
    "dataloader_counter = DataLoader(cf_success_ds, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9b3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    # Create an integrated gradients object.\n",
    "    # integrated_gradients = IntegratedGradients(model)\n",
    "    # Generated attributions on integrated gradients\n",
    "    attributions = np.vstack(\n",
    "        [\n",
    "            integrated_gradients.attribute(\n",
    "                real.to(device),\n",
    "                target=target.to(device),\n",
    "                baselines=counterfactual.to(device),\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            for (real, target), (counterfactual, _) in zip(\n",
    "                dataloader_real, dataloader_counter\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387ba61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e843e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for creating an interactive visualization of our attributions\n",
    "model.cpu()\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"viridis\")\n",
    "colors = cmap([0, 255])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_classifications(image, counter, hybrid):\n",
    "    model.eval()\n",
    "    class_idx = [full_dataset.classes.index(c) for c in classes]\n",
    "    tensor = torch.from_numpy(np.stack([image, counter, hybrid])).float()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)[:, class_idx]\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred, counter_pred, hybrid_pred = probs\n",
    "    return pred.numpy(), counter_pred.numpy(), hybrid_pred.numpy()\n",
    "\n",
    "\n",
    "def visualize_counterfactuals(idx, threshold=0.1):\n",
    "    image = real_success_ds[idx][0].numpy()\n",
    "    counter = cf_success_ds[idx][0].numpy()\n",
    "    mask = get_mask(attributions[idx], threshold)\n",
    "    hybrid = (1 - mask) * image + mask * counter\n",
    "    nan_mask = copy.deepcopy(mask)\n",
    "    nan_mask[nan_mask != 0] = 1\n",
    "    nan_mask[nan_mask == 0] = np.nan\n",
    "    # PLOT\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"\"\"\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   mmm.ooo.ccc.hhh\n",
    "                                   ....ggg.fff.ppp\n",
    "                                   \"\"\",\n",
    "        figsize=(20, 5),\n",
    "    )\n",
    "    # Original\n",
    "    viz.visualize_image_attr(\n",
    "        np.transpose(mask, (1, 2, 0)),\n",
    "        np.transpose(image, (1, 2, 0)),\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"absolute_value\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Mask\",\n",
    "        use_pyplot=False,\n",
    "        plt_fig_axis=(fig, axes[\"m\"]),\n",
    "    )\n",
    "    # Original\n",
    "    axes[\"o\"].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axes[\"o\"].set_title(\"Original\", fontsize=24)\n",
    "    # Counterfactual\n",
    "    axes[\"c\"].imshow(counter.squeeze(), cmap=\"gray\")\n",
    "    axes[\"c\"].set_title(\"Counterfactual\", fontsize=24)\n",
    "    # Hybrid\n",
    "    axes[\"h\"].imshow(hybrid.squeeze(), cmap=\"gray\")\n",
    "    axes[\"h\"].set_title(\"Hybrid\", fontsize=24)\n",
    "    # Mask\n",
    "    pred, counter_pred, hybrid_pred = get_classifications(image, counter, hybrid)\n",
    "    axes[\"g\"].barh(classes, pred, color=colors)\n",
    "    axes[\"f\"].barh(classes, counter_pred, color=colors)\n",
    "    axes[\"p\"].barh(classes, hybrid_pred, color=colors)\n",
    "    for ix in [\"m\", \"o\", \"c\", \"h\"]:\n",
    "        axes[ix].axis(\"off\")\n",
    "\n",
    "    for ix in [\"g\", \"f\", \"p\"]:\n",
    "        for tick in axes[ix].get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "        axes[ix].set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d2a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 5.2: Observing the effect of the changes on the classifier</h3>\n",
    "Below is a small widget to interact with the above analysis. As you change the `threshold`, see how the prediction of the hybrid changes.\n",
    "At what point does it swap over?\n",
    "\n",
    "If you want to see different samples, slide through the `idx`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f878a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interact(visualize_counterfactuals, idx=(0, 99), threshold=(0.0, 1.0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aceac4",
   "metadata": {},
   "source": [
    "HELP!!! Interactive (still!) doesn't work. No worries... uncomment the following cell and choose your index and threshold by typing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae84d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your own adventure\n",
    "# idx = 0\n",
    "# threshold = 0.1\n",
    "\n",
    "# # Plotting :)\n",
    "# visualize_counterfactuals(idx, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5ceb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<h4>Questions</h4>\n",
    "\n",
    "- Can you find features that define either of the two classes?\n",
    "-  How consistent are they across the samples?\n",
    "-  Is there a range of thresholds where most of the hybrids swap over to the target class? (If you want to see that area, try to change the range of thresholds in the slider by setting `threshold=(minimum_value, maximum_value, step_size)`\n",
    "\n",
    "Feel free to discuss your answers on the exercise chat!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca976c6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1>The End.</h1>\n",
    "    Go forth and train some GANs!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96b144",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going Further\n",
    "\n",
    "Here are some ideas for how to continue with this notebook:\n",
    "\n",
    "1. Improve the classifier. This code uses a VGG network for the classification. On the synapse dataset, we will get a validation accuracy of around 80%. Try to see if you can improve the classifier accuracy.\n",
    "    * (easy) Data augmentation: The training code for the classifier is quite simple in this example. Enlarge the amount of available training data by adding augmentations (transpose and mirror the images, add noise, change the intensity, etc.).\n",
    "    * (easy) Network architecture: The VGG network has a few parameters that one can tune. Try a few to see what difference it makes.\n",
    "    * (easy) Inspect the classifier predictions: Take random samples from the test dataset and classify them. Show the images together with their predicted and actual labels.\n",
    "    * (medium) Other networks:  Try different architectures (e.g., a [ResNet](https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/#resnet-from-scratch)) and see if the accuracy can be improved.\n",
    "\n",
    "2. Explore the CycleGAN.\n",
    "    * (easy) The example code below shows how to translate between GABA and acetylcholine. Try different combinations. Can you start to see differences between some pairs of classes? Which are the ones where the differences are the most or the least obvious? Can you see any differences that aren't well described by the mask? How would you describe these?\n",
    "\n",
    "3. Try on your own data!\n",
    "    * Have a look at how the synapse images are organized in `data/raw/synapses`. Copy the directory structure and use your own images. Depending on your data, you might have to adjust the image size (128x128 for the synapses) and number of channels in the VGG network and CycleGAN code."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
